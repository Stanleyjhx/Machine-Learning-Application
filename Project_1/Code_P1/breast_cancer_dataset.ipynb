{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "breast_cancer_dataset.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "qiktq3ofql0d"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SVP6T3vlrpHu"
      },
      "source": [
        "# Mini Project 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bHI5w4gN_ABw"
      },
      "source": [
        "# Breast Cancer Wisconsin (Diagnostic) Data Set\n",
        "https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+(Diagnostic)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vFYelb1rylz0"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "#the output of plotting commands is displayed inline within frontends\n",
        "%matplotlib inline                                  \n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#it is important to set the seed for reproducibility as it initializes the random number generator\n",
        "np.random.seed(1234)"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QzJDo0inp2cY"
      },
      "source": [
        "## Data processing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "cOR3TPQgq583",
        "outputId": "981bd6c8-9da4-4bae-f949-3d969cbf222a"
      },
      "source": [
        "df = pd.read_csv(\"/content/breast_cancer_wisconsin.csv\")\n",
        "df.drop_duplicates(subset=None, keep='first', inplace=True) # remove duplicated rows\n",
        "df = df[~df.eq('?').any(1)] # remove missing data \n",
        "df = df.reset_index(drop=True)\n",
        "df"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>Clump_Thickness</th>\n",
              "      <th>Uniformity_of_Cell_Size</th>\n",
              "      <th>Uniformity_of_Cell_Shape</th>\n",
              "      <th>Marginal_Adhesion</th>\n",
              "      <th>Single_Epithelial_Cell_Size</th>\n",
              "      <th>Bare_Nuclei</th>\n",
              "      <th>Bland_Chromatin</th>\n",
              "      <th>Normal_Nucleoli</th>\n",
              "      <th>Mitoses</th>\n",
              "      <th>Class</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1000025</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1002945</td>\n",
              "      <td>5</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>5</td>\n",
              "      <td>7</td>\n",
              "      <td>10</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1015425</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1016277</td>\n",
              "      <td>6</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1017023</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>670</th>\n",
              "      <td>776715</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>671</th>\n",
              "      <td>841769</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>672</th>\n",
              "      <td>888820</td>\n",
              "      <td>5</td>\n",
              "      <td>10</td>\n",
              "      <td>10</td>\n",
              "      <td>3</td>\n",
              "      <td>7</td>\n",
              "      <td>3</td>\n",
              "      <td>8</td>\n",
              "      <td>10</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>673</th>\n",
              "      <td>897471</td>\n",
              "      <td>4</td>\n",
              "      <td>8</td>\n",
              "      <td>6</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>10</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>674</th>\n",
              "      <td>897471</td>\n",
              "      <td>4</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>5</td>\n",
              "      <td>4</td>\n",
              "      <td>5</td>\n",
              "      <td>10</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>675 rows Ã— 11 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "          id  Clump_Thickness  ...  Mitoses  Class\n",
              "0    1000025                5  ...        1      2\n",
              "1    1002945                5  ...        1      2\n",
              "2    1015425                3  ...        1      2\n",
              "3    1016277                6  ...        1      2\n",
              "4    1017023                4  ...        1      2\n",
              "..       ...              ...  ...      ...    ...\n",
              "670   776715                3  ...        1      2\n",
              "671   841769                2  ...        1      2\n",
              "672   888820                5  ...        2      4\n",
              "673   897471                4  ...        1      4\n",
              "674   897471                4  ...        1      4\n",
              "\n",
              "[675 rows x 11 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 433
        },
        "id": "1-f3LpC0AvwW",
        "outputId": "be44f106-b2ef-46c1-ffd2-3c585c5cf6e8"
      },
      "source": [
        "means = df.drop(columns=[\"id\"]).groupby(by=[\"Class\"]).mean()\n",
        "\n",
        "#visualization of the data\n",
        "label_names = list(means.columns)\n",
        "for i, label in enumerate(label_names):\n",
        "    print(i + 1, label)\n",
        "labels = [i for i in range(1, 9)]\n",
        "\n",
        "class2_means = means.iloc[0]\n",
        "class4_means = means.iloc[1]\n",
        "\n",
        "x = np.arange(len(labels))  # the label locations\n",
        "width = 0.35  # the width of the bars\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "rects1 = ax.bar(x - width/2, class2_means, width, label='class 2')\n",
        "rects2 = ax.bar(x + width/2, class4_means, width, label='class 4')\n",
        "\n",
        "# Add some text for labels, title and custom x-axis tick labels, etc.\n",
        "ax.set_ylabel('Mean Values')\n",
        "# ax.set_title('')\n",
        "ax.set_xticks(x)\n",
        "ax.set_xticklabels(labels)\n",
        "ax.legend()\n",
        "fig.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1 Clump_Thickness\n",
            "2 Uniformity_of_Cell_Size\n",
            "3 Uniformity_of_Cell_Shape\n",
            "4 Marginal_Adhesion\n",
            "5 Single_Epithelial_Cell_Size\n",
            "6 Bland_Chromatin\n",
            "7 Normal_Nucleoli\n",
            "8 Mitoses\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "id": "uWu7gTJWhndd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3d262551-01b6-40e0-81f9-c508ce727fcd"
      },
      "source": [
        "x, y = df.loc[:,\"Clump_Thickness\":\"Mitoses\"], df.loc[:,[\"Class\"]]                   #slices the first two columns or features from the data\n",
        "\n",
        "#print the feature shape and classes of dataset \n",
        "(N,D), C = x.shape, df[\"Class\"].unique().size\n",
        "print(f'instances (N) \\t {N} \\n features (D) \\t {D} \\n classes (C) \\t {C}')\n",
        "\n",
        "train_num = int(N * 0.7)\n",
        "val_num = int(N * 0.85)\n",
        "inds = np.random.permutation(N)                                                     #generates an indices array from 0 to N-1 and permutes it \n",
        "\n",
        "#split the dataset into train, validation and test\n",
        "x_train, y_train = x.loc[inds[:train_num],], y.loc[inds[:train_num],].to_numpy(dtype=np.int16).flatten()\n",
        "x_val, y_val = x.loc[inds[train_num:val_num],], y.loc[inds[train_num:val_num],].to_numpy(dtype=np.int16).flatten()\n",
        "x_test, y_test = x.loc[inds[val_num:],], y.loc[inds[val_num:],].to_numpy(dtype=np.int16).flatten()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "instances (N) \t 675 \n",
            " features (D) \t 9 \n",
            " classes (C) \t 2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BuXp0MufIe72"
      },
      "source": [
        "## KNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SX0Oul88hndj"
      },
      "source": [
        "#define the metric we will use to measure similarity\n",
        "#if the input shapes are [1,N1,F] and [N2,1,F] then output shape is [N2,N1]\n",
        "#as numpy supports broadcasting with arithmetic operations\n",
        "#for more on numpy broadcasting refer to: https://numpy.org/doc/stable/user/basics.broadcasting.html   \n",
        "euclidean = lambda x1, x2: np.sqrt(np.sum((x1 - x2)**2, axis=-1))\n",
        "manhattan = lambda x1, x2: np.sum(np.abs(x1 - x2), axis=-1)\n",
        "\n",
        "class KNN:\n",
        "\n",
        "    def __init__(self, K=1, dist_fn= euclidean):\n",
        "        self.dist_fn = dist_fn\n",
        "        self.K = K\n",
        "        return\n",
        "    \n",
        "    def fit(self, x, y):\n",
        "        ''' Store the training data using this method as it is a lazy learner'''\n",
        "        self.x = x\n",
        "        self.y = y\n",
        "        self.C = df[\"Class\"].unique().size\n",
        "        return self\n",
        "    \n",
        "    def predict(self, x_test):\n",
        "        ''' Makes a prediction using the stored training data and the test data given as argument'''\n",
        "        num_test = x_test.shape[0]\n",
        "        #calculate distance between the training & test samples and returns an array of shape [num_test, num_train]\n",
        "        distances = self.dist_fn(self.x[None,:,:], x_test[:,None,:])\n",
        "        #ith-row of knns stores the indices of k closest training samples to the ith-test sample \n",
        "        knns = np.zeros((num_test, self.K), dtype=int)\n",
        "        #ith-row of y_prob has the probability distribution over C classes\n",
        "        y_prob = np.zeros((num_test, self.C))\n",
        "        for i in range(num_test):\n",
        "            knns[i,:] = np.argsort(distances[i])[:self.K]\n",
        "            #counts the number of instances of each class in the K-closest training samples\n",
        "            count_arr = np.bincount(self.y[knns[i,:]], minlength=5)\n",
        "            y_prob[i,:] = [count_arr[2], count_arr[4]] \n",
        "        #y_prob /= np.sum(y_prob, axis=-1, keepdims=True)\n",
        "        #simply divide by K to get a probability distribution\n",
        "        y_prob /= self.K\n",
        "        return y_prob, knns"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w00vPRHXycu2",
        "outputId": "52536d8e-1f55-445b-dbfb-5a70ec5323df"
      },
      "source": [
        "model = KNN(K=3)\n",
        "\n",
        "y_prob, knns = model.fit(x_train.to_numpy(dtype=np.int16), y_train).predict(x_test.to_numpy(dtype=np.int16))\n",
        "print('knns shape:', knns.shape)\n",
        "print('y_prob shape:', y_prob.shape)\n",
        "\n",
        "#To get hard predictions by choosing the class with the maximum probability\n",
        "y_pred = np.argmax(y_prob,axis=-1)\n",
        "for i in range(len(y_pred)):\n",
        "    if y_pred[i] == 0:\n",
        "        y_pred[i] = 2\n",
        "    else:\n",
        "        y_pred[i] = 4\n",
        "accuracy = np.sum(y_pred == y_test)/y_test.shape[0]\n",
        "\n",
        "print(f'accuracy is {accuracy*100:.1f}.')\n",
        "\n",
        "#boolean array to later slice the indexes of correct and incorrect predictions\n",
        "correct = y_test == y_pred\n",
        "incorrect = np.logical_not(correct)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "knns shape: (102, 3)\n",
            "y_prob shape: (102, 2)\n",
            "accuracy is 96.1.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pjrp1JQioin1"
      },
      "source": [
        "def evaluate_acc(K):\n",
        "    model = KNN(K)\n",
        "    y_prob, knns = model.fit(x_train.to_numpy(dtype=np.float), y_train).predict(x_test.to_numpy(dtype=np.float))\n",
        "    y_pred = np.argmax(y_prob, axis=-1)\n",
        "    for i in range(len(y_pred)):\n",
        "        if y_pred[i] == 0:\n",
        "            y_pred[i] = 2\n",
        "        else:\n",
        "            y_pred[i] = 4\n",
        "    accuracy = np.sum(y_pred == y_test) / y_test.shape[0]\n",
        "    print(f'accuracy is {accuracy * 100:.1f}.')\n",
        "    return accuracy\n",
        "\n",
        "# #define a function for the MSE loss\n",
        "# loss = lambda y, yh: np.mean((y-yh)**2)\n",
        "\n",
        "# def mse(K):\n",
        "#     model = KNN(K)\n",
        "#     y_prob, knns = model.fit(x_train.to_numpy(dtype=np.float), y_train).predict(x_val.to_numpy(dtype=np.float))\n",
        "#     y_pred = np.argmax(y_prob, axis=-1)\n",
        "#     for i in range(len(y_pred)):\n",
        "#         if y_pred[i] == 0:\n",
        "#             y_pred[i] = 2\n",
        "#         else:\n",
        "#             y_pred[i] = 4\n",
        "#     return loss(y_pred, y_val)\n",
        "\n",
        "def evaluate_acc_val(K):\n",
        "    model = KNN(K)\n",
        "    y_prob, knns = model.fit(x_train.to_numpy(dtype=np.float), y_train).predict(x_val.to_numpy(dtype=np.float))\n",
        "    y_pred = np.argmax(y_prob, axis=-1)\n",
        "    for i in range(len(y_pred)):\n",
        "        if y_pred[i] == 0:\n",
        "            y_pred[i] = 2\n",
        "        else:\n",
        "            y_pred[i] = 4\n",
        "    accuracy = np.sum(y_pred == y_val) / y_val.shape[0]\n",
        "    # print(f'accuracy is {accuracy * 100:.1f}.')\n",
        "    return accuracy"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "id": "8ViUnbOWoioG",
        "outputId": "d17b2e06-351b-442f-8957-60617b197a30"
      },
      "source": [
        "K_list = range(1,30)\n",
        "err_valid = []\n",
        "for K in K_list:\n",
        "    err_valid.append(evaluate_acc_val(K))\n",
        "    \n",
        "plt.plot(K_list, err_valid, label='valid')\n",
        "\n",
        "plt.legend()\n",
        "plt.xlabel('K (number of neighbours)')\n",
        "plt.ylabel('Validation set accuarcy')\n",
        "plt.ylim(0.8, 1)\n",
        "plt.show()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_GeA5K20oioH",
        "outputId": "cd80cbf0-78b0-44d2-d45a-d0f822b036d3"
      },
      "source": [
        "accuracy = evaluate_acc(3)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "accuracy is 96.1.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-w6v4h2NEaTV"
      },
      "source": [
        "## Decision tree"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JT7FjodLEcBl"
      },
      "source": [
        "class Node:\n",
        "    def __init__(self, data_indices, parent):\n",
        "        self.data_indices = data_indices                    #stores the data indices which are in the region defined by this node\n",
        "        self.left = None                                    #stores the left child of the node \n",
        "        self.right = None                                   #stores the right child of the node\n",
        "        self.split_feature = None                           #the feature for split at this node\n",
        "        self.split_value = None                             #the value of the feature for split at this node\n",
        "        if parent:\n",
        "            self.depth = parent.depth + 1                   #obtain the dept of the node by adding one to dept of the parent \n",
        "            self.num_classes = parent.num_classes           #copies the num classes from the parent \n",
        "            self.data = parent.data                         #copies the data from the parent\n",
        "            self.labels = parent.labels                     #copies the labels from the parent\n",
        "            class_prob = np.bincount(self.labels[data_indices], minlength=self.num_classes) #this is counting frequency of different labels in the region defined by this node\n",
        "            self.class_prob = class_prob / np.sum(class_prob)  #stores the class probability for the node\n",
        "            #note that we'll use the class probabilites of the leaf nodes for making predictions after the tree is built"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "054oFQRGElqY"
      },
      "source": [
        "def greedy_test(node, cost_fn):\n",
        "    if type(node.data) != np.ndarray:\n",
        "        node.data = node.data.to_numpy(dtype=np.int16)\n",
        "    #initialize the best parameter values\n",
        "    best_cost = np.inf\n",
        "    best_feature, best_value = None, None\n",
        "    num_instances, num_features = node.data.shape\n",
        "    #sort the features to get the test value candidates by taking the average of consecutive sorted feature values \n",
        "    data_sorted = np.sort(node.data[node.data_indices],axis=0)\n",
        "    test_candidates = (data_sorted[1:] + data_sorted[:-1]) / 2.\n",
        "    for f in range(num_features):\n",
        "        #stores the data corresponding to the f-th feature\n",
        "        data_f = node.data[node.data_indices, f]\n",
        "        for test in test_candidates[:,f]:\n",
        "            #Split the indices using the test value of f-th feature\n",
        "            left_indices = node.data_indices[data_f <= test]\n",
        "            right_indices = node.data_indices[data_f > test]\n",
        "            #we can't have a split where a child has zero element\n",
        "            #if this is true over all the test features and their test values  then the function returns the best cost as infinity\n",
        "            if len(left_indices) == 0 or len(right_indices) == 0:                \n",
        "                continue\n",
        "            #compute the left and right cost based on the current split                                                         \n",
        "            left_cost = cost_fn(node.labels[left_indices])\n",
        "            right_cost = cost_fn(node.labels[right_indices])\n",
        "            num_left, num_right = left_indices.shape[0], right_indices.shape[0]\n",
        "            #get the combined cost using the weighted sum of left and right cost\n",
        "            cost = (num_left * left_cost + num_right * right_cost)/num_instances\n",
        "            #update only when a lower cost is encountered\n",
        "            if cost < best_cost:\n",
        "                best_cost = cost\n",
        "                best_feature = f\n",
        "                best_value = test\n",
        "    return best_cost, best_feature, best_value"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qd3pktrxFGfQ"
      },
      "source": [
        "#computes misclassification cost by subtracting the maximum probability of any class\n",
        "def cost_misclassification(labels):\n",
        "    counts = np.bincount(labels) \n",
        "    class_probs = counts / np.sum(counts)\n",
        "    #you could compress both the steps above by doing class_probs = np.bincount(labels) / len(labels)\n",
        "    return 1 - np.max(class_probs)\n",
        "\n",
        "#computes entropy of the labels by computing the class probabilities\n",
        "def cost_entropy(labels):\n",
        "    class_probs = np.bincount(labels) / len(labels)\n",
        "    class_probs = class_probs[class_probs > 0]              #this steps is remove 0 probabilities for removing numerical issues while computing log\n",
        "    return -np.sum(class_probs * np.log(class_probs))       #expression for entropy -\\sigma p(x)log[p(x)]\n",
        "\n",
        "#computes the gini index cost\n",
        "def cost_gini_index(labels):\n",
        "    class_probs = np.bincount(labels) / len(labels)\n",
        "    return 1 - np.sum(np.square(class_probs))               #expression for gini index 1-\\sigma p(x)^2"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Gi_hNaAImtY"
      },
      "source": [
        "class DecisionTree:\n",
        "    def __init__(self, num_classes=None, max_depth=3, cost_fn=cost_misclassification, min_leaf_instances=1):\n",
        "        self.max_depth = max_depth      #maximum dept for termination \n",
        "        self.root = None                #stores the root of the decision tree \n",
        "        self.cost_fn = cost_fn          #stores the cost function of the decision tree \n",
        "        self.num_classes = num_classes  #stores the total number of classes\n",
        "        self.min_leaf_instances = min_leaf_instances  #minimum number of instances in a leaf for termination\n",
        "        \n",
        "    def fit(self, data, labels):\n",
        "        pass                            #pass in python 3 means nothing happens and the method here is empty\n",
        "    \n",
        "    def predict(self, data_test):\n",
        "        pass"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ODj52cfNJLjD"
      },
      "source": [
        "def fit(self, data, labels):\n",
        "    self.data = data\n",
        "    self.labels = labels\n",
        "    if self.num_classes is None:\n",
        "        self.num_classes = np.max(labels) + 1\n",
        "    #below are initialization of the root of the decision tree\n",
        "    self.root = Node(np.arange(data.shape[0]), None)\n",
        "    self.root.data = data\n",
        "    self.root.labels = labels\n",
        "    self.root.num_classes = self.num_classes\n",
        "    self.root.depth = 0\n",
        "    #to recursively build the rest of the tree\n",
        "    self._fit_tree(self.root)\n",
        "    return self\n",
        "\n",
        "def _fit_tree(self, node):\n",
        "    #This gives the condition for termination of the recursion resulting in a leaf node\n",
        "    if node.depth == self.max_depth or len(node.data_indices) <= self.min_leaf_instances:\n",
        "        return\n",
        "    #greedily select the best test by minimizing the cost\n",
        "    cost, split_feature, split_value = greedy_test(node, self.cost_fn)\n",
        "    #if the cost returned is infinity it means that it is not possible to split the node and hence terminate\n",
        "    if np.isinf(cost):\n",
        "        return\n",
        "    #print(f'best feature: {split_feature}, value {split_value}, cost {cost}')\n",
        "    #to get a boolean array suggesting which data indices corresponding to this node are in the left of the split\n",
        "    test = node.data[node.data_indices,split_feature] <= split_value\n",
        "    #store the split feature and value of the node\n",
        "    node.split_feature = split_feature\n",
        "    node.split_value = split_value\n",
        "    #define new nodes which are going to be the left and right child of the present node\n",
        "    left = Node(node.data_indices[test], node)\n",
        "    right = Node(node.data_indices[np.logical_not(test)], node)\n",
        "    #recursive call to the _fit_tree()\n",
        "    self._fit_tree(left)\n",
        "    self._fit_tree(right)\n",
        "    #assign the left and right child to present child\n",
        "    node.left = left\n",
        "    node.right = right\n",
        "\n",
        "DecisionTree.fit = fit\n",
        "DecisionTree._fit_tree = _fit_tree"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v9VFStJ_JSTL"
      },
      "source": [
        "def predict(self, data_test):\n",
        "    data_test = data_test.to_numpy(dtype=np.int16)\n",
        "    class_probs = np.zeros((data_test.shape[0], self.num_classes))\n",
        "    for n, x in enumerate(data_test):\n",
        "        node = self.root\n",
        "        #loop along the dept of the tree looking region where the present data sample fall in based on the split feature and value\n",
        "        while node.left:\n",
        "            if x[node.split_feature] <= node.split_value:\n",
        "                node = node.left\n",
        "            else:\n",
        "                node = node.right\n",
        "        #the loop terminates when you reach a leaf of the tree and the class probability of that node is taken for prediction\n",
        "        class_probs[n,:] = node.class_prob\n",
        "    return class_probs\n",
        "\n",
        "DecisionTree.predict = predict"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cNY1sF0zT-jn",
        "outputId": "3e81d013-706d-4126-d689-0ab16dbc4bf7"
      },
      "source": [
        "tree = DecisionTree(max_depth=6)\n",
        "probs_test = tree.fit(x_train, y_train).predict(x_test)\n",
        "y_pred = np.argmax(probs_test,1)\n",
        "accuracy = np.sum(y_pred == y_test)/y_test.shape[0]\n",
        "print(f'accuracy is {accuracy*100:.1f}.')"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "accuracy is 96.1.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0mZY5io4qLsB"
      },
      "source": [
        "def evaluate_acc(d):\n",
        "    tree = DecisionTree(max_depth=d)\n",
        "    probs_test = tree.fit(x_train, y_train).predict(x_test)\n",
        "    y_pred = np.argmax(probs_test,1)\n",
        "    accuracy = np.sum(y_pred == y_test)/y_test.shape[0]\n",
        "    print(f'accuracy is {accuracy*100:.1f}.')\n",
        "    return accuracy\n",
        "\n",
        "# #define a function for the MSE loss\n",
        "# loss = lambda y, yh: np.mean((y-yh)**2)\n",
        "\n",
        "# def mse(max_depth):\n",
        "#     tree = DecisionTree(max_depth)\n",
        "#     probs_test = tree.fit(x_train, y_train).predict(x_val)\n",
        "#     y_pred = np.argmax(probs_test,1)\n",
        "#     return loss(y_pred, y_val)\n",
        "\n",
        "def evaluate_acc_val(d):\n",
        "    tree = DecisionTree(max_depth=d)\n",
        "    probs_test = tree.fit(x_train, y_train).predict(x_val)\n",
        "    y_pred = np.argmax(probs_test,1)\n",
        "    accuracy = np.sum(y_pred == y_val)/y_val.shape[0]\n",
        "    # print(f'accuracy is {accuracy*100:.1f}.')\n",
        "    return accuracy"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WdlOPnnXqq8Z"
      },
      "source": [
        "depth_list = range(1,30)\n",
        "err_valid = []\n",
        "for d in depth_list:\n",
        "    err_valid.append(evaluate_acc_val(d))\n",
        "    \n",
        "plt.plot(depth_list, err_valid, label='valid')\n",
        "\n",
        "plt.legend()\n",
        "plt.xlabel('Tree depth')\n",
        "plt.ylabel('Validation set accuracy')\n",
        "plt.ylim(0.8, 1)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tNWIf0mDJXBs"
      },
      "source": [
        "evaluate_acc(6)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_hMSbrZua0pA"
      },
      "source": [
        "''' Decision boundary '''\n",
        "y_train2 = np.copy(y_train)\n",
        "for i in range(0, len(y_train2)):\n",
        "    if y_train2[i] == 2:\n",
        "        y_train2[i] = 0\n",
        "    elif y_train2[i] == 4:\n",
        "        y_train2[i] = 1\n",
        "x_all = x\n",
        "model = DecisionTree(max_depth=6)\n",
        "y_train_prob = np.zeros((y_train.shape[0], 2))\n",
        "y_train_prob[np.arange(y_train.shape[0]), y_train2] = 1\n",
        "y_prob_all = model.fit(x_train, y_train).predict(x_all)\n",
        "z = np.zeros((y_train_prob.shape[0],1), dtype=np.int16)\n",
        "y_train_prob2 = np.append(y_train_prob, z, axis=1)\n",
        "plt.scatter(x_train.iloc[:,0], x_train.iloc[:,1], c=y_train_prob2, marker='o', alpha=1)\n",
        "plt.scatter(x_all.iloc[:,0], x_all.iloc[:,1], c=y_prob_all[:, [2, 4, 0]], marker='.', alpha=.01)\n",
        "plt.ylabel('Uniformity of Cell Size')\n",
        "plt.xlabel('Clump thickness')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}