{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "mlp_test_with_training.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hdgMMZEBGqyl"
      },
      "source": [
        "Now import TensorFlow and the module dependencies:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tQuiyC7ve96S",
        "outputId": "3114cdaa-9964-4907-db9b-8990fac062ca"
      },
      "source": [
        "!pip install tensorflow==2.4.1\n",
        "!pip install gpustat\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from keras.utils.np_utils import to_categorical\n",
        "import time"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorflow==2.4.1 in /usr/local/lib/python3.7/dist-packages (2.4.1)\n",
            "Requirement already satisfied: astunparse~=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.4.1) (1.6.3)\n",
            "Requirement already satisfied: numpy~=1.19.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.4.1) (1.19.5)\n",
            "Requirement already satisfied: absl-py~=0.10 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.4.1) (0.12.0)\n",
            "Requirement already satisfied: google-pasta~=0.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.4.1) (0.2.0)\n",
            "Requirement already satisfied: tensorboard~=2.4 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.4.1) (2.4.1)\n",
            "Requirement already satisfied: flatbuffers~=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.4.1) (1.12)\n",
            "Requirement already satisfied: typing-extensions~=3.7.4 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.4.1) (3.7.4.3)\n",
            "Requirement already satisfied: tensorflow-estimator<2.5.0,>=2.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.4.1) (2.4.0)\n",
            "Requirement already satisfied: keras-preprocessing~=1.1.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.4.1) (1.1.2)\n",
            "Requirement already satisfied: grpcio~=1.32.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.4.1) (1.32.0)\n",
            "Requirement already satisfied: wrapt~=1.12.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.4.1) (1.12.1)\n",
            "Requirement already satisfied: wheel~=0.35 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.4.1) (0.36.2)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.4.1) (3.12.4)\n",
            "Requirement already satisfied: gast==0.3.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.4.1) (0.3.3)\n",
            "Requirement already satisfied: six~=1.15.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.4.1) (1.15.0)\n",
            "Requirement already satisfied: h5py~=2.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.4.1) (2.10.0)\n",
            "Requirement already satisfied: opt-einsum~=3.3.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.4.1) (3.3.0)\n",
            "Requirement already satisfied: termcolor~=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.4.1) (1.1.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow==2.4.1) (3.3.4)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow==2.4.1) (1.8.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow==2.4.1) (1.0.1)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow==2.4.1) (54.2.0)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow==2.4.1) (1.28.0)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow==2.4.1) (2.23.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow==2.4.1) (0.4.3)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard~=2.4->tensorflow==2.4.1) (3.8.1)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow==2.4.1) (4.2.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow==2.4.1) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.6\" in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow==2.4.1) (4.7.2)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow==2.4.1) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow==2.4.1) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow==2.4.1) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow==2.4.1) (3.0.4)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow==2.4.1) (1.3.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard~=2.4->tensorflow==2.4.1) (3.4.1)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow==2.4.1) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow==2.4.1) (3.1.0)\n",
            "Collecting gpustat\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b4/69/d8c849715171aeabd61af7da080fdc60948b5a396d2422f1f4672e43d008/gpustat-0.6.0.tar.gz (78kB)\n",
            "\u001b[K     |████████████████████████████████| 81kB 4.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: six>=1.7 in /usr/local/lib/python3.7/dist-packages (from gpustat) (1.15.0)\n",
            "Requirement already satisfied: nvidia-ml-py3>=7.352.0 in /usr/local/lib/python3.7/dist-packages (from gpustat) (7.352.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.7/dist-packages (from gpustat) (5.4.8)\n",
            "Collecting blessings>=1.6\n",
            "  Downloading https://files.pythonhosted.org/packages/03/74/489f85a78247609c6b4f13733cbf3ba0d864b11aa565617b645d6fdf2a4a/blessings-1.7-py3-none-any.whl\n",
            "Building wheels for collected packages: gpustat\n",
            "  Building wheel for gpustat (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gpustat: filename=gpustat-0.6.0-cp37-none-any.whl size=12621 sha256=004ebf6f92f008156373326e07ae5f39c61c83b9f9a4be7bc9ad25f820a3aefb\n",
            "  Stored in directory: /root/.cache/pip/wheels/48/b4/d5/fb5b7f1d040f2ff20687e3bad6867d63155dbde5a7c10f4293\n",
            "Successfully built gpustat\n",
            "Installing collected packages: blessings, gpustat\n",
            "Successfully installed blessings-1.7 gpustat-0.6.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WcCM3Oh1Eef1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d4101121-74f9-41f0-9cd9-73f53268995c"
      },
      "source": [
        "# data normalization\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
        "# Rescale the images from [0,255] to the [0.0,1.0] range.\n",
        "x_train, x_test = x_train[..., np.newaxis]/255.0, x_test[..., np.newaxis]/255.0\n",
        "\n",
        "y_train = to_categorical(y_train)\n",
        "y_test = to_categorical(y_test)\n",
        "\n",
        "x_train = x_train - np.mean(x_train)\n",
        "x_test = x_test - np.mean(x_test)\n",
        "\n",
        "\n",
        "# reshape\n",
        "x_train = np.reshape(x_train, (60000, 784))\n",
        "x_test = np.reshape(x_test, (10000, 784))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zATVIFMhBOXf"
      },
      "source": [
        "\n",
        "class MLP_0():\n",
        "\n",
        "  # initialization\n",
        "  def __init__(self, sizes, epochs=100, l_rate=0.001):\n",
        "      self.sizes = sizes\n",
        "      self.epochs = epochs\n",
        "      self.l_rate = l_rate\n",
        "      self.params = self.initialization()\n",
        "\n",
        "  def softmax(self, x, derivative=False):\n",
        "          exps = np.exp(x - x.max())\n",
        "          if derivative:\n",
        "              return exps / np.sum(exps, axis=0) * (1 - exps / np.sum(exps, axis=0))\n",
        "          return exps / np.sum(exps, axis=0)\n",
        "\n",
        "  def sigmoid(self, x, derivative=False):\n",
        "          if derivative:\n",
        "              return (np.exp(-x))/((np.exp(-x)+1)**2)\n",
        "          return 1/(1 + np.exp(-x))\n",
        "\n",
        "\n",
        "  # with no hidden layer\n",
        "  def initialization(self):\n",
        "          # nums of computing units in each layer\n",
        "          input_layer=self.sizes[0]\n",
        "          output_layer=self.sizes[1]\n",
        "          params = {\n",
        "              'W0':np.random.randn(output_layer, input_layer) * np.sqrt(1. / output_layer)\n",
        "          }\n",
        "          return params\n",
        "\n",
        "  def forward(self, x_train):\n",
        "          params = self.params\n",
        "          params['A0'] = x_train\n",
        "          # direct from input layer to output layer\n",
        "          params['A1']=np.dot(params['W0'], params['A0'])\n",
        "          #softmax at the end of classification\n",
        "          # note, after I change sigmoid to softmax, the accuracy drops\n",
        "          params['S1'] =self.softmax(params['A1'])\n",
        "          return params['S1']\n",
        "\n",
        "\n",
        "  def backward(self, y_train, output):\n",
        "          params = self.params\n",
        "          changes_to_w = {}\n",
        "          # Calculate W1 update\n",
        "          error = 2 * (output - y_train) / output.shape[0]  *self.softmax(params['A1'], derivative=True)\n",
        "          changes_to_w['W0'] = np.outer(error, params['A0'])\n",
        "          return changes_to_w\n",
        "\n",
        "\n",
        "  def update_network_parameters(self, changes_to_w):        \n",
        "          for key, value in changes_to_w.items():\n",
        "              self.params[key] -= self.l_rate * value\n",
        "\n",
        "  def train(self, x_train, y_train, x_val, y_val):\n",
        "          start_time = time.time()\n",
        "          for iteration in range(self.epochs):\n",
        "              for x,y in zip(x_train, y_train):\n",
        "                  output = self.forward(x)\n",
        "                  changes_to_w = self.backward(y, output)\n",
        "                  self.update_network_parameters(changes_to_w)\n",
        "              \n",
        "              accuracy = self.evaluate_acc(x_val, y_val)\n",
        "              print('Epoch: {0}, Time Spent: {1:.2f}s, Accuracy: {2:.2f}%'.format(\n",
        "                  iteration+1, time.time() - start_time, accuracy * 100\n",
        "              ))\n",
        "  def evaluate_acc(self, x_val, y_val):\n",
        "          predictions = []\n",
        "\n",
        "          for x, y in zip(x_val, y_val):\n",
        "              output = self.forward(x)\n",
        "              pred = np.argmax(output)\n",
        "              predictions.append(pred == np.argmax(y))\n",
        "          \n",
        "          return np.mean(predictions)\n",
        "\n",
        "\n",
        "  \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kh-lrT2LG8l_",
        "outputId": "2909bc78-ba12-4566-f894-5d6e5eb9e9ed"
      },
      "source": [
        "mlp0 = MLP_0(sizes=[784,10])\n",
        "mlp0.train(x_train,y_train,x_train,y_train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 1, Time Spent: 6.53s, Accuracy: 12.08%\n",
            "Epoch: 2, Time Spent: 13.00s, Accuracy: 26.24%\n",
            "Epoch: 3, Time Spent: 19.45s, Accuracy: 39.92%\n",
            "Epoch: 4, Time Spent: 25.95s, Accuracy: 50.52%\n",
            "Epoch: 5, Time Spent: 32.41s, Accuracy: 57.69%\n",
            "Epoch: 6, Time Spent: 38.93s, Accuracy: 63.66%\n",
            "Epoch: 7, Time Spent: 45.39s, Accuracy: 67.98%\n",
            "Epoch: 8, Time Spent: 51.90s, Accuracy: 71.22%\n",
            "Epoch: 9, Time Spent: 58.37s, Accuracy: 73.46%\n",
            "Epoch: 10, Time Spent: 64.88s, Accuracy: 75.38%\n",
            "Epoch: 11, Time Spent: 71.45s, Accuracy: 76.88%\n",
            "Epoch: 12, Time Spent: 77.94s, Accuracy: 78.05%\n",
            "Epoch: 13, Time Spent: 84.40s, Accuracy: 79.02%\n",
            "Epoch: 14, Time Spent: 90.90s, Accuracy: 79.88%\n",
            "Epoch: 15, Time Spent: 97.40s, Accuracy: 80.58%\n",
            "Epoch: 16, Time Spent: 104.02s, Accuracy: 81.20%\n",
            "Epoch: 17, Time Spent: 110.52s, Accuracy: 81.71%\n",
            "Epoch: 18, Time Spent: 117.00s, Accuracy: 82.18%\n",
            "Epoch: 19, Time Spent: 123.49s, Accuracy: 82.61%\n",
            "Epoch: 20, Time Spent: 130.00s, Accuracy: 83.00%\n",
            "Epoch: 21, Time Spent: 136.56s, Accuracy: 83.39%\n",
            "Epoch: 22, Time Spent: 143.09s, Accuracy: 83.71%\n",
            "Epoch: 23, Time Spent: 149.60s, Accuracy: 83.96%\n",
            "Epoch: 24, Time Spent: 156.10s, Accuracy: 84.24%\n",
            "Epoch: 25, Time Spent: 162.62s, Accuracy: 84.53%\n",
            "Epoch: 26, Time Spent: 169.11s, Accuracy: 84.80%\n",
            "Epoch: 27, Time Spent: 175.60s, Accuracy: 84.97%\n",
            "Epoch: 28, Time Spent: 182.09s, Accuracy: 85.15%\n",
            "Epoch: 29, Time Spent: 188.57s, Accuracy: 85.34%\n",
            "Epoch: 30, Time Spent: 195.08s, Accuracy: 85.52%\n",
            "Epoch: 31, Time Spent: 201.61s, Accuracy: 85.69%\n",
            "Epoch: 32, Time Spent: 208.13s, Accuracy: 85.84%\n",
            "Epoch: 33, Time Spent: 214.58s, Accuracy: 86.00%\n",
            "Epoch: 34, Time Spent: 221.08s, Accuracy: 86.17%\n",
            "Epoch: 35, Time Spent: 227.55s, Accuracy: 86.31%\n",
            "Epoch: 36, Time Spent: 234.02s, Accuracy: 86.43%\n",
            "Epoch: 37, Time Spent: 240.50s, Accuracy: 86.52%\n",
            "Epoch: 38, Time Spent: 247.00s, Accuracy: 86.66%\n",
            "Epoch: 39, Time Spent: 253.53s, Accuracy: 86.78%\n",
            "Epoch: 40, Time Spent: 260.04s, Accuracy: 86.89%\n",
            "Epoch: 41, Time Spent: 266.56s, Accuracy: 86.98%\n",
            "Epoch: 42, Time Spent: 273.06s, Accuracy: 87.08%\n",
            "Epoch: 43, Time Spent: 279.54s, Accuracy: 87.19%\n",
            "Epoch: 44, Time Spent: 286.04s, Accuracy: 87.28%\n",
            "Epoch: 45, Time Spent: 292.57s, Accuracy: 87.38%\n",
            "Epoch: 46, Time Spent: 299.06s, Accuracy: 87.47%\n",
            "Epoch: 47, Time Spent: 305.51s, Accuracy: 87.56%\n",
            "Epoch: 48, Time Spent: 312.06s, Accuracy: 87.65%\n",
            "Epoch: 49, Time Spent: 318.56s, Accuracy: 87.72%\n",
            "Epoch: 50, Time Spent: 325.10s, Accuracy: 87.78%\n",
            "Epoch: 51, Time Spent: 331.61s, Accuracy: 87.85%\n",
            "Epoch: 52, Time Spent: 338.10s, Accuracy: 87.91%\n",
            "Epoch: 53, Time Spent: 344.61s, Accuracy: 87.97%\n",
            "Epoch: 54, Time Spent: 351.15s, Accuracy: 88.04%\n",
            "Epoch: 55, Time Spent: 357.66s, Accuracy: 88.13%\n",
            "Epoch: 56, Time Spent: 364.17s, Accuracy: 88.19%\n",
            "Epoch: 57, Time Spent: 370.70s, Accuracy: 88.25%\n",
            "Epoch: 58, Time Spent: 377.19s, Accuracy: 88.30%\n",
            "Epoch: 59, Time Spent: 383.67s, Accuracy: 88.35%\n",
            "Epoch: 60, Time Spent: 390.25s, Accuracy: 88.42%\n",
            "Epoch: 61, Time Spent: 396.80s, Accuracy: 88.48%\n",
            "Epoch: 62, Time Spent: 403.35s, Accuracy: 88.53%\n",
            "Epoch: 63, Time Spent: 409.90s, Accuracy: 88.58%\n",
            "Epoch: 64, Time Spent: 416.44s, Accuracy: 88.63%\n",
            "Epoch: 65, Time Spent: 422.95s, Accuracy: 88.68%\n",
            "Epoch: 66, Time Spent: 429.52s, Accuracy: 88.72%\n",
            "Epoch: 67, Time Spent: 436.09s, Accuracy: 88.80%\n",
            "Epoch: 68, Time Spent: 442.59s, Accuracy: 88.84%\n",
            "Epoch: 69, Time Spent: 449.10s, Accuracy: 88.88%\n",
            "Epoch: 70, Time Spent: 455.57s, Accuracy: 88.92%\n",
            "Epoch: 71, Time Spent: 462.03s, Accuracy: 88.95%\n",
            "Epoch: 72, Time Spent: 468.83s, Accuracy: 89.00%\n",
            "Epoch: 73, Time Spent: 476.71s, Accuracy: 89.04%\n",
            "Epoch: 74, Time Spent: 483.42s, Accuracy: 89.09%\n",
            "Epoch: 75, Time Spent: 489.98s, Accuracy: 89.12%\n",
            "Epoch: 76, Time Spent: 496.49s, Accuracy: 89.16%\n",
            "Epoch: 77, Time Spent: 502.99s, Accuracy: 89.18%\n",
            "Epoch: 78, Time Spent: 509.53s, Accuracy: 89.21%\n",
            "Epoch: 79, Time Spent: 516.06s, Accuracy: 89.26%\n",
            "Epoch: 80, Time Spent: 522.55s, Accuracy: 89.29%\n",
            "Epoch: 81, Time Spent: 529.09s, Accuracy: 89.32%\n",
            "Epoch: 82, Time Spent: 535.57s, Accuracy: 89.35%\n",
            "Epoch: 83, Time Spent: 542.07s, Accuracy: 89.37%\n",
            "Epoch: 84, Time Spent: 548.55s, Accuracy: 89.40%\n",
            "Epoch: 85, Time Spent: 555.07s, Accuracy: 89.44%\n",
            "Epoch: 86, Time Spent: 561.57s, Accuracy: 89.46%\n",
            "Epoch: 87, Time Spent: 568.04s, Accuracy: 89.50%\n",
            "Epoch: 88, Time Spent: 574.49s, Accuracy: 89.53%\n",
            "Epoch: 89, Time Spent: 580.95s, Accuracy: 89.56%\n",
            "Epoch: 90, Time Spent: 587.41s, Accuracy: 89.58%\n",
            "Epoch: 91, Time Spent: 593.86s, Accuracy: 89.61%\n",
            "Epoch: 92, Time Spent: 600.33s, Accuracy: 89.63%\n",
            "Epoch: 93, Time Spent: 606.79s, Accuracy: 89.67%\n",
            "Epoch: 94, Time Spent: 613.26s, Accuracy: 89.70%\n",
            "Epoch: 95, Time Spent: 619.73s, Accuracy: 89.72%\n",
            "Epoch: 96, Time Spent: 626.18s, Accuracy: 89.74%\n",
            "Epoch: 97, Time Spent: 632.66s, Accuracy: 89.75%\n",
            "Epoch: 98, Time Spent: 639.12s, Accuracy: 89.78%\n",
            "Epoch: 99, Time Spent: 645.61s, Accuracy: 89.80%\n",
            "Epoch: 100, Time Spent: 652.06s, Accuracy: 89.82%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FeHnfNJao2Mv"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ajg9ECdowuvz"
      },
      "source": [
        "class MLP_1():\n",
        "\n",
        "    def __init__(self, sizes, epochs=500, l_rate=0.0001):\n",
        "          self.sizes = sizes\n",
        "          self.epochs = epochs\n",
        "          self.l_rate = l_rate\n",
        "          # we save all parameters in the neural network in this dictionary\n",
        "          self.params = self.initialization()\n",
        "\n",
        "    def softmax(self, x, derivative=False):\n",
        "        # Numerically stable with large exponentials\n",
        "        exps = np.exp(x - x.max())\n",
        "        if derivative:\n",
        "            return exps / np.sum(exps, axis=0) * (1 - exps / np.sum(exps, axis=0))\n",
        "        return exps / np.sum(exps, axis=0)\n",
        "\n",
        "    def initialization(self):\n",
        "        # number of nodes in each layer\n",
        "        input_layer = self.sizes[0]\n",
        "        hidden_1 = self.sizes[1]\n",
        "        output_layer = self.sizes[2]\n",
        "\n",
        "        params = {\n",
        "            'W1': np.random.randn(hidden_1, input_layer) * np.sqrt(1. / hidden_1),\n",
        "            'W2': np.random.randn(output_layer, hidden_1) * np.sqrt(1. / output_layer)\n",
        "        }\n",
        "\n",
        "        return params\n",
        "\n",
        "    def forward_pass(self, x_train):\n",
        "        params = self.params\n",
        "\n",
        "        # input layer activations becomes sample\n",
        "        params['A0'] = x_train\n",
        "\n",
        "        # input layer to hidden layer 1\n",
        "        params['Z1'] = np.dot(params[\"W1\"], params['A0'])\n",
        "        params['A1'] = self.ReLU(params['Z1'])\n",
        "        # hidden layer 1 to hidden layer 2\n",
        "        params['Z2'] = np.dot(params[\"W2\"], params['A1'])\n",
        "        params['A2'] = self.softmax(params['Z2'])\n",
        "\n",
        "\n",
        "        return params['A2']\n",
        "\n",
        "    def backward_pass(self, y_train, output):\n",
        "        '''\n",
        "            This is the backpropagation algorithm, for calculating the updates\n",
        "            of the neural network's parameters.\n",
        "\n",
        "            Note: There is a stability issue that causes warnings. This is\n",
        "                  caused  by the dot and multiply operations on the huge arrays.\n",
        "\n",
        "                  RuntimeWarning: invalid value encountered in true_divide\n",
        "                  RuntimeWarning: overflow encountered in exp\n",
        "                  RuntimeWarning: overflow encountered in square\n",
        "        '''\n",
        "        params = self.params\n",
        "        change_w = {}\n",
        "\n",
        "        # Calculate W2 update\n",
        "        error = 2 * (output - y_train) / output.shape[0] * self.softmax(params['Z2'], derivative=True)\n",
        "        change_w['W2'] = np.outer(error, params['A1'])\n",
        "        # Calculate W1 update\n",
        "        error = np.dot(params['W2'].T, error) * self.ReLU(params['Z1'], derivative=True)\n",
        "        change_w['W1'] = np.outer(error, params['A0'])\n",
        "\n",
        "        return change_w\n",
        "\n",
        "    def update_network_parameters(self, changes_to_w):\n",
        "        '''\n",
        "            Update network parameters according to update rule from\n",
        "            Stochastic Gradient Descent.\n",
        "\n",
        "            θ = θ - η * ∇J(x, y),\n",
        "                theta θ:            a network parameter (e.g. a weight w)\n",
        "                eta η:              the learning rate\n",
        "                gradient ∇J(x, y):  the gradient of the objective function,\n",
        "                                    i.e. the change for a specific theta θ\n",
        "        '''\n",
        "\n",
        "        for key, value in changes_to_w.items():\n",
        "            self.params[key] -= self.l_rate * value\n",
        "\n",
        "    def compute_accuracy(self, x_val, y_val):\n",
        "        '''\n",
        "            This function does a forward pass of x, then checks if the indices\n",
        "            of the maximum value in the output equals the indices in the label\n",
        "            y. Then it sums over each prediction and calculates the accuracy.\n",
        "        '''\n",
        "        predictions = []\n",
        "\n",
        "        for x, y in zip(x_val, y_val):\n",
        "            output = self.forward_pass(x)\n",
        "            pred = np.argmax(output)\n",
        "            predictions.append(pred == np.argmax(y))\n",
        "\n",
        "        return np.mean(predictions)\n",
        "\n",
        "    def train(self, x_train, y_train, x_val, y_val):\n",
        "        start_time = time.time()\n",
        "        for iteration in range(self.epochs):\n",
        "            for x, y in zip(x_train, y_train):\n",
        "                output = self.forward_pass(x)\n",
        "                changes_to_w = self.backward_pass(y, output)\n",
        "                self.update_network_parameters(changes_to_w)\n",
        "\n",
        "            accuracy = self.compute_accuracy(x_val, y_val)\n",
        "            print('Epoch: {0}, Time Spent: {1:.2f}s, Accuracy: {2:.2f}%'.format(\n",
        "                iteration + 1, time.time() - start_time, accuracy * 100\n",
        "            ))\n",
        "    def ReLU(self, x, derivative = False):\n",
        "            if derivative:\n",
        "              y = x\n",
        "              y[y <= 0] = 0\n",
        "              y[y > 0] = 1\n",
        "              return y\n",
        "            x[x <= 0] = 0\n",
        "            return x\n",
        "\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "zrs9bLavxgwe",
        "outputId": "900cc6d5-2f02-412d-c991-caa81cb5b62d"
      },
      "source": [
        "mlp1 = MLP_1(sizes=[784,128,10])\n",
        "mlp1.train(x_train,y_train,x_train,y_train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 1, Time Spent: 45.63s, Accuracy: 21.39%\n",
            "Epoch: 2, Time Spent: 90.91s, Accuracy: 34.76%\n",
            "Epoch: 3, Time Spent: 136.21s, Accuracy: 42.83%\n",
            "Epoch: 4, Time Spent: 181.45s, Accuracy: 50.78%\n",
            "Epoch: 5, Time Spent: 226.66s, Accuracy: 58.15%\n",
            "Epoch: 6, Time Spent: 271.74s, Accuracy: 63.56%\n",
            "Epoch: 7, Time Spent: 317.07s, Accuracy: 67.53%\n",
            "Epoch: 8, Time Spent: 362.47s, Accuracy: 70.59%\n",
            "Epoch: 9, Time Spent: 409.08s, Accuracy: 72.94%\n",
            "Epoch: 10, Time Spent: 455.68s, Accuracy: 74.87%\n",
            "Epoch: 11, Time Spent: 502.09s, Accuracy: 76.44%\n",
            "Epoch: 12, Time Spent: 548.86s, Accuracy: 77.69%\n",
            "Epoch: 13, Time Spent: 595.37s, Accuracy: 78.78%\n",
            "Epoch: 14, Time Spent: 641.80s, Accuracy: 79.74%\n",
            "Epoch: 15, Time Spent: 688.48s, Accuracy: 80.50%\n",
            "Epoch: 16, Time Spent: 734.95s, Accuracy: 81.25%\n",
            "Epoch: 17, Time Spent: 781.45s, Accuracy: 81.87%\n",
            "Epoch: 18, Time Spent: 827.85s, Accuracy: 82.49%\n",
            "Epoch: 19, Time Spent: 874.11s, Accuracy: 82.93%\n",
            "Epoch: 20, Time Spent: 920.80s, Accuracy: 83.40%\n",
            "Epoch: 21, Time Spent: 967.08s, Accuracy: 83.79%\n",
            "Epoch: 22, Time Spent: 1015.68s, Accuracy: 84.20%\n",
            "Epoch: 23, Time Spent: 1062.29s, Accuracy: 84.62%\n",
            "Epoch: 24, Time Spent: 1108.78s, Accuracy: 84.97%\n",
            "Epoch: 25, Time Spent: 1155.50s, Accuracy: 85.25%\n",
            "Epoch: 26, Time Spent: 1202.03s, Accuracy: 85.52%\n",
            "Epoch: 27, Time Spent: 1248.46s, Accuracy: 85.77%\n",
            "Epoch: 28, Time Spent: 1294.63s, Accuracy: 85.99%\n",
            "Epoch: 29, Time Spent: 1341.02s, Accuracy: 86.19%\n",
            "Epoch: 30, Time Spent: 1387.06s, Accuracy: 86.39%\n",
            "Epoch: 31, Time Spent: 1433.29s, Accuracy: 86.61%\n",
            "Epoch: 32, Time Spent: 1479.47s, Accuracy: 86.78%\n",
            "Epoch: 33, Time Spent: 1525.38s, Accuracy: 86.94%\n",
            "Epoch: 34, Time Spent: 1571.44s, Accuracy: 87.09%\n",
            "Epoch: 35, Time Spent: 1617.37s, Accuracy: 87.24%\n",
            "Epoch: 36, Time Spent: 1663.39s, Accuracy: 87.37%\n",
            "Epoch: 37, Time Spent: 1709.42s, Accuracy: 87.53%\n",
            "Epoch: 38, Time Spent: 1755.51s, Accuracy: 87.69%\n",
            "Epoch: 39, Time Spent: 1801.42s, Accuracy: 87.79%\n",
            "Epoch: 40, Time Spent: 1847.47s, Accuracy: 87.91%\n",
            "Epoch: 41, Time Spent: 1893.49s, Accuracy: 88.03%\n",
            "Epoch: 42, Time Spent: 1939.46s, Accuracy: 88.15%\n",
            "Epoch: 43, Time Spent: 1985.30s, Accuracy: 88.25%\n",
            "Epoch: 44, Time Spent: 2031.44s, Accuracy: 88.35%\n",
            "Epoch: 45, Time Spent: 2077.36s, Accuracy: 88.43%\n",
            "Epoch: 46, Time Spent: 2123.36s, Accuracy: 88.51%\n",
            "Epoch: 47, Time Spent: 2169.24s, Accuracy: 88.62%\n",
            "Epoch: 48, Time Spent: 2215.38s, Accuracy: 88.73%\n",
            "Epoch: 49, Time Spent: 2261.37s, Accuracy: 88.80%\n",
            "Epoch: 50, Time Spent: 2307.36s, Accuracy: 88.88%\n",
            "Epoch: 51, Time Spent: 2353.50s, Accuracy: 88.94%\n",
            "Epoch: 52, Time Spent: 2399.62s, Accuracy: 89.00%\n",
            "Epoch: 53, Time Spent: 2446.42s, Accuracy: 89.08%\n",
            "Epoch: 54, Time Spent: 2492.63s, Accuracy: 89.13%\n",
            "Epoch: 55, Time Spent: 2538.47s, Accuracy: 89.19%\n",
            "Epoch: 56, Time Spent: 2586.96s, Accuracy: 89.27%\n",
            "Epoch: 57, Time Spent: 2633.16s, Accuracy: 89.35%\n",
            "Epoch: 58, Time Spent: 2679.13s, Accuracy: 89.43%\n",
            "Epoch: 59, Time Spent: 2725.00s, Accuracy: 89.50%\n",
            "Epoch: 60, Time Spent: 2770.83s, Accuracy: 89.56%\n",
            "Epoch: 61, Time Spent: 2816.63s, Accuracy: 89.63%\n",
            "Epoch: 62, Time Spent: 2862.62s, Accuracy: 89.71%\n",
            "Epoch: 63, Time Spent: 2908.42s, Accuracy: 89.77%\n",
            "Epoch: 64, Time Spent: 2954.47s, Accuracy: 89.83%\n",
            "Epoch: 65, Time Spent: 3000.31s, Accuracy: 89.87%\n",
            "Epoch: 66, Time Spent: 3046.39s, Accuracy: 89.93%\n",
            "Epoch: 67, Time Spent: 3092.34s, Accuracy: 90.01%\n",
            "Epoch: 68, Time Spent: 3138.13s, Accuracy: 90.07%\n",
            "Epoch: 69, Time Spent: 3184.18s, Accuracy: 90.14%\n",
            "Epoch: 70, Time Spent: 3230.12s, Accuracy: 90.19%\n",
            "Epoch: 71, Time Spent: 3276.24s, Accuracy: 90.24%\n",
            "Epoch: 72, Time Spent: 3322.18s, Accuracy: 90.28%\n",
            "Epoch: 73, Time Spent: 3368.12s, Accuracy: 90.34%\n",
            "Epoch: 74, Time Spent: 3414.33s, Accuracy: 90.38%\n",
            "Epoch: 75, Time Spent: 3460.32s, Accuracy: 90.43%\n",
            "Epoch: 76, Time Spent: 3506.37s, Accuracy: 90.48%\n",
            "Epoch: 77, Time Spent: 3552.43s, Accuracy: 90.54%\n",
            "Epoch: 78, Time Spent: 3598.17s, Accuracy: 90.57%\n",
            "Epoch: 79, Time Spent: 3643.84s, Accuracy: 90.61%\n",
            "Epoch: 80, Time Spent: 3689.54s, Accuracy: 90.65%\n",
            "Epoch: 81, Time Spent: 3735.33s, Accuracy: 90.68%\n",
            "Epoch: 82, Time Spent: 3781.13s, Accuracy: 90.72%\n",
            "Epoch: 83, Time Spent: 3826.84s, Accuracy: 90.76%\n",
            "Epoch: 84, Time Spent: 3872.48s, Accuracy: 90.81%\n",
            "Epoch: 85, Time Spent: 3917.88s, Accuracy: 90.84%\n",
            "Epoch: 86, Time Spent: 3963.03s, Accuracy: 90.88%\n",
            "Epoch: 87, Time Spent: 4008.17s, Accuracy: 90.92%\n",
            "Epoch: 88, Time Spent: 4053.13s, Accuracy: 90.94%\n",
            "Epoch: 89, Time Spent: 4098.25s, Accuracy: 90.97%\n",
            "Epoch: 90, Time Spent: 4143.24s, Accuracy: 91.01%\n",
            "Epoch: 91, Time Spent: 4188.35s, Accuracy: 91.05%\n",
            "Epoch: 92, Time Spent: 4233.38s, Accuracy: 91.10%\n",
            "Epoch: 93, Time Spent: 4278.62s, Accuracy: 91.12%\n",
            "Epoch: 94, Time Spent: 4323.56s, Accuracy: 91.14%\n",
            "Epoch: 95, Time Spent: 4368.59s, Accuracy: 91.18%\n",
            "Epoch: 96, Time Spent: 4413.64s, Accuracy: 91.21%\n",
            "Epoch: 97, Time Spent: 4459.02s, Accuracy: 91.25%\n",
            "Epoch: 98, Time Spent: 4504.42s, Accuracy: 91.29%\n",
            "Epoch: 99, Time Spent: 4549.47s, Accuracy: 91.32%\n",
            "Epoch: 100, Time Spent: 4594.65s, Accuracy: 91.35%\n",
            "Epoch: 101, Time Spent: 4639.84s, Accuracy: 91.39%\n",
            "Epoch: 102, Time Spent: 4684.86s, Accuracy: 91.42%\n",
            "Epoch: 103, Time Spent: 4730.02s, Accuracy: 91.45%\n",
            "Epoch: 104, Time Spent: 4775.07s, Accuracy: 91.48%\n",
            "Epoch: 105, Time Spent: 4820.55s, Accuracy: 91.51%\n",
            "Epoch: 106, Time Spent: 4865.81s, Accuracy: 91.53%\n",
            "Epoch: 107, Time Spent: 4911.03s, Accuracy: 91.54%\n",
            "Epoch: 108, Time Spent: 4956.30s, Accuracy: 91.57%\n",
            "Epoch: 109, Time Spent: 5001.32s, Accuracy: 91.59%\n",
            "Epoch: 110, Time Spent: 5046.32s, Accuracy: 91.63%\n",
            "Epoch: 111, Time Spent: 5091.52s, Accuracy: 91.65%\n",
            "Epoch: 112, Time Spent: 5136.66s, Accuracy: 91.69%\n",
            "Epoch: 113, Time Spent: 5181.85s, Accuracy: 91.72%\n",
            "Epoch: 114, Time Spent: 5226.82s, Accuracy: 91.76%\n",
            "Epoch: 115, Time Spent: 5271.93s, Accuracy: 91.80%\n",
            "Epoch: 116, Time Spent: 5316.88s, Accuracy: 91.83%\n",
            "Epoch: 117, Time Spent: 5362.07s, Accuracy: 91.85%\n",
            "Epoch: 118, Time Spent: 5407.39s, Accuracy: 91.86%\n",
            "Epoch: 119, Time Spent: 5452.36s, Accuracy: 91.88%\n",
            "Epoch: 120, Time Spent: 5497.43s, Accuracy: 91.94%\n",
            "Epoch: 121, Time Spent: 5542.59s, Accuracy: 91.97%\n",
            "Epoch: 122, Time Spent: 5587.82s, Accuracy: 92.00%\n",
            "Epoch: 123, Time Spent: 5633.22s, Accuracy: 92.02%\n",
            "Epoch: 124, Time Spent: 5678.38s, Accuracy: 92.04%\n",
            "Epoch: 125, Time Spent: 5723.53s, Accuracy: 92.06%\n",
            "Epoch: 126, Time Spent: 5768.61s, Accuracy: 92.08%\n",
            "Epoch: 127, Time Spent: 5813.81s, Accuracy: 92.11%\n",
            "Epoch: 128, Time Spent: 5859.10s, Accuracy: 92.12%\n",
            "Epoch: 129, Time Spent: 5904.10s, Accuracy: 92.14%\n",
            "Epoch: 130, Time Spent: 5949.23s, Accuracy: 92.16%\n",
            "Epoch: 131, Time Spent: 5994.27s, Accuracy: 92.19%\n",
            "Epoch: 132, Time Spent: 6039.09s, Accuracy: 92.20%\n",
            "Epoch: 133, Time Spent: 6084.03s, Accuracy: 92.24%\n",
            "Epoch: 134, Time Spent: 6129.19s, Accuracy: 92.25%\n",
            "Epoch: 135, Time Spent: 6174.32s, Accuracy: 92.25%\n",
            "Epoch: 136, Time Spent: 6219.16s, Accuracy: 92.27%\n",
            "Epoch: 137, Time Spent: 6264.14s, Accuracy: 92.30%\n",
            "Epoch: 138, Time Spent: 6309.18s, Accuracy: 92.32%\n",
            "Epoch: 139, Time Spent: 6354.34s, Accuracy: 92.34%\n",
            "Epoch: 140, Time Spent: 6399.47s, Accuracy: 92.35%\n",
            "Epoch: 141, Time Spent: 6444.45s, Accuracy: 92.37%\n",
            "Epoch: 142, Time Spent: 6489.59s, Accuracy: 92.38%\n",
            "Epoch: 143, Time Spent: 6534.60s, Accuracy: 92.41%\n",
            "Epoch: 144, Time Spent: 6579.66s, Accuracy: 92.42%\n",
            "Epoch: 145, Time Spent: 6624.70s, Accuracy: 92.43%\n",
            "Epoch: 146, Time Spent: 6669.72s, Accuracy: 92.45%\n",
            "Epoch: 147, Time Spent: 6714.81s, Accuracy: 92.46%\n",
            "Epoch: 148, Time Spent: 6759.73s, Accuracy: 92.48%\n",
            "Epoch: 149, Time Spent: 6804.71s, Accuracy: 92.50%\n",
            "Epoch: 150, Time Spent: 6849.56s, Accuracy: 92.52%\n",
            "Epoch: 151, Time Spent: 6894.60s, Accuracy: 92.53%\n",
            "Epoch: 152, Time Spent: 6939.86s, Accuracy: 92.55%\n",
            "Epoch: 153, Time Spent: 6984.98s, Accuracy: 92.56%\n",
            "Epoch: 154, Time Spent: 7030.05s, Accuracy: 92.58%\n",
            "Epoch: 155, Time Spent: 7075.28s, Accuracy: 92.60%\n",
            "Epoch: 156, Time Spent: 7120.36s, Accuracy: 92.61%\n",
            "Epoch: 157, Time Spent: 7165.60s, Accuracy: 92.63%\n",
            "Epoch: 158, Time Spent: 7210.65s, Accuracy: 92.65%\n",
            "Epoch: 159, Time Spent: 7255.73s, Accuracy: 92.66%\n",
            "Epoch: 160, Time Spent: 7300.82s, Accuracy: 92.68%\n",
            "Epoch: 161, Time Spent: 7345.74s, Accuracy: 92.70%\n",
            "Epoch: 162, Time Spent: 7390.81s, Accuracy: 92.72%\n",
            "Epoch: 163, Time Spent: 7435.90s, Accuracy: 92.73%\n",
            "Epoch: 164, Time Spent: 7481.07s, Accuracy: 92.76%\n",
            "Epoch: 165, Time Spent: 7525.95s, Accuracy: 92.77%\n",
            "Epoch: 166, Time Spent: 7570.95s, Accuracy: 92.79%\n",
            "Epoch: 167, Time Spent: 7616.06s, Accuracy: 92.82%\n",
            "Epoch: 168, Time Spent: 7661.07s, Accuracy: 92.84%\n",
            "Epoch: 169, Time Spent: 7706.05s, Accuracy: 92.85%\n",
            "Epoch: 170, Time Spent: 7751.22s, Accuracy: 92.87%\n",
            "Epoch: 171, Time Spent: 7796.16s, Accuracy: 92.89%\n",
            "Epoch: 172, Time Spent: 7841.19s, Accuracy: 92.90%\n",
            "Epoch: 173, Time Spent: 7886.17s, Accuracy: 92.92%\n",
            "Epoch: 174, Time Spent: 7931.26s, Accuracy: 92.94%\n",
            "Epoch: 175, Time Spent: 7976.37s, Accuracy: 92.95%\n",
            "Epoch: 176, Time Spent: 8021.38s, Accuracy: 92.97%\n",
            "Epoch: 177, Time Spent: 8066.41s, Accuracy: 92.98%\n",
            "Epoch: 178, Time Spent: 8111.38s, Accuracy: 93.00%\n",
            "Epoch: 179, Time Spent: 8156.40s, Accuracy: 93.01%\n",
            "Epoch: 180, Time Spent: 8201.58s, Accuracy: 93.03%\n",
            "Epoch: 181, Time Spent: 8246.83s, Accuracy: 93.04%\n",
            "Epoch: 182, Time Spent: 8292.01s, Accuracy: 93.05%\n",
            "Epoch: 183, Time Spent: 8337.22s, Accuracy: 93.06%\n",
            "Epoch: 184, Time Spent: 8382.32s, Accuracy: 93.08%\n",
            "Epoch: 185, Time Spent: 8427.22s, Accuracy: 93.09%\n",
            "Epoch: 186, Time Spent: 8472.28s, Accuracy: 93.11%\n",
            "Epoch: 187, Time Spent: 8517.14s, Accuracy: 93.12%\n",
            "Epoch: 188, Time Spent: 8562.20s, Accuracy: 93.13%\n",
            "Epoch: 189, Time Spent: 8607.16s, Accuracy: 93.15%\n",
            "Epoch: 190, Time Spent: 8652.00s, Accuracy: 93.16%\n",
            "Epoch: 191, Time Spent: 8697.16s, Accuracy: 93.18%\n",
            "Epoch: 192, Time Spent: 8742.09s, Accuracy: 93.19%\n",
            "Epoch: 193, Time Spent: 8787.02s, Accuracy: 93.20%\n",
            "Epoch: 194, Time Spent: 8832.21s, Accuracy: 93.22%\n",
            "Epoch: 195, Time Spent: 8877.02s, Accuracy: 93.22%\n",
            "Epoch: 196, Time Spent: 8921.94s, Accuracy: 93.23%\n",
            "Epoch: 197, Time Spent: 8967.13s, Accuracy: 93.25%\n",
            "Epoch: 198, Time Spent: 9012.04s, Accuracy: 93.26%\n",
            "Epoch: 199, Time Spent: 9057.09s, Accuracy: 93.27%\n",
            "Epoch: 200, Time Spent: 9102.13s, Accuracy: 93.28%\n",
            "Epoch: 201, Time Spent: 9147.08s, Accuracy: 93.29%\n",
            "Epoch: 202, Time Spent: 9192.38s, Accuracy: 93.30%\n",
            "Epoch: 203, Time Spent: 9237.43s, Accuracy: 93.31%\n",
            "Epoch: 204, Time Spent: 9282.31s, Accuracy: 93.31%\n",
            "Epoch: 205, Time Spent: 9327.23s, Accuracy: 93.33%\n",
            "Epoch: 206, Time Spent: 9372.29s, Accuracy: 93.34%\n",
            "Epoch: 207, Time Spent: 9417.39s, Accuracy: 93.35%\n",
            "Epoch: 208, Time Spent: 9462.70s, Accuracy: 93.37%\n",
            "Epoch: 209, Time Spent: 9507.73s, Accuracy: 93.38%\n",
            "Epoch: 210, Time Spent: 9552.89s, Accuracy: 93.39%\n",
            "Epoch: 211, Time Spent: 9597.92s, Accuracy: 93.40%\n",
            "Epoch: 212, Time Spent: 9643.23s, Accuracy: 93.41%\n",
            "Epoch: 213, Time Spent: 9688.51s, Accuracy: 93.42%\n",
            "Epoch: 214, Time Spent: 9733.59s, Accuracy: 93.43%\n",
            "Epoch: 215, Time Spent: 9778.72s, Accuracy: 93.44%\n",
            "Epoch: 216, Time Spent: 9823.77s, Accuracy: 93.45%\n",
            "Epoch: 217, Time Spent: 9868.63s, Accuracy: 93.46%\n",
            "Epoch: 218, Time Spent: 9913.73s, Accuracy: 93.47%\n",
            "Epoch: 219, Time Spent: 9958.82s, Accuracy: 93.48%\n",
            "Epoch: 220, Time Spent: 10003.95s, Accuracy: 93.49%\n",
            "Epoch: 221, Time Spent: 10049.07s, Accuracy: 93.50%\n",
            "Epoch: 222, Time Spent: 10094.18s, Accuracy: 93.50%\n",
            "Epoch: 223, Time Spent: 10139.15s, Accuracy: 93.52%\n",
            "Epoch: 224, Time Spent: 10184.27s, Accuracy: 93.53%\n",
            "Epoch: 225, Time Spent: 10229.23s, Accuracy: 93.54%\n",
            "Epoch: 226, Time Spent: 10274.24s, Accuracy: 93.54%\n",
            "Epoch: 227, Time Spent: 10319.25s, Accuracy: 93.55%\n",
            "Epoch: 228, Time Spent: 10364.23s, Accuracy: 93.56%\n",
            "Epoch: 229, Time Spent: 10409.44s, Accuracy: 93.58%\n",
            "Epoch: 230, Time Spent: 10454.39s, Accuracy: 93.59%\n",
            "Epoch: 231, Time Spent: 10499.48s, Accuracy: 93.60%\n",
            "Epoch: 232, Time Spent: 10544.70s, Accuracy: 93.61%\n",
            "Epoch: 233, Time Spent: 10589.87s, Accuracy: 93.63%\n",
            "Epoch: 234, Time Spent: 10635.01s, Accuracy: 93.64%\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-5a471172725a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmlp1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMLP_1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msizes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m784\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmlp1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-3-8b73d1339cdc>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, x_train, y_train, x_val, y_val)\u001b[0m\n\u001b[1;32m    102\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0miteration\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m                 \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_pass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m                 \u001b[0mchanges_to_w\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward_pass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_network_parameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchanges_to_w\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-8b73d1339cdc>\u001b[0m in \u001b[0;36mforward_pass\u001b[0;34m(self, x_train)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0;31m# input layer to hidden layer 1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m         \u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Z1'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"W1\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'A0'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m         \u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'A1'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mReLU\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Z1'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0;31m# hidden layer 1 to hidden layer 2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mdot\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2kje7DUKRhGU"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 351
        },
        "id": "Q2LDGlKYJ9UD",
        "outputId": "0c4c04e4-01b3-402e-f36c-eea2ea1bb786"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import xlwt\n",
        "import os\n",
        "\n",
        "with open('mlp1.txt','r+') as fd:\n",
        "  e = list(range(1,100))\n",
        "  p = []\n",
        "  #read all accuracy from the performance report\n",
        "  for text in fd.readlines():\n",
        "    y=text.split(':')[3].strip(\"\\n\").strip('%').strip()\n",
        "    p.append(y.split(\"\\n\"))\n",
        "\n",
        "  p = list(np.concatenate(p).flat)\n",
        "  p = [round(float(i)/100,3) for i in p]\n",
        "  print(p)\n",
        "  print(e)\n",
        "\n",
        "fd.close()\n",
        "\n",
        "plt.plot(e, p, color='green',  linewidth = 2)\n",
        "  \n",
        "# setting x and y axis range\n",
        "plt.ylim(0,1)\n",
        "plt.xlim(1,100)\n",
        "  \n",
        "# naming the x axis\n",
        "plt.xlabel('Iterations')\n",
        "# naming the y axis\n",
        "plt.ylabel('Accuracy')\n",
        "  \n",
        "# giving a title to my graph\n",
        "plt.title('One hidden layer MLP test with training set performance')\n",
        "  \n",
        "# function to show the plot\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.214, 0.348, 0.428, 0.508, 0.582, 0.636, 0.675, 0.706, 0.729, 0.749, 0.764, 0.777, 0.788, 0.797, 0.805, 0.812, 0.819, 0.825, 0.829, 0.834, 0.838, 0.842, 0.846, 0.85, 0.853, 0.855, 0.858, 0.86, 0.862, 0.864, 0.866, 0.868, 0.869, 0.871, 0.872, 0.874, 0.875, 0.877, 0.878, 0.879, 0.88, 0.882, 0.882, 0.883, 0.884, 0.885, 0.886, 0.887, 0.888, 0.889, 0.889, 0.89, 0.891, 0.891, 0.892, 0.893, 0.893, 0.894, 0.895, 0.896, 0.896, 0.897, 0.898, 0.898, 0.899, 0.899, 0.9, 0.901, 0.901, 0.902, 0.902, 0.903, 0.903, 0.904, 0.904, 0.905, 0.905, 0.906, 0.906, 0.907, 0.907, 0.907, 0.908, 0.908, 0.908, 0.909, 0.909, 0.909, 0.91, 0.91, 0.91, 0.911, 0.911, 0.911, 0.912, 0.912, 0.912, 0.913, 0.913]\n",
            "[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dJKUKcuNa92z"
      },
      "source": [
        "\n",
        "class MLP_2():\n",
        "\n",
        "  # initialization\n",
        "  def __init__(self, sizes, epochs=100, l_rate=0.001):\n",
        "      self.sizes = sizes\n",
        "      self.epochs = epochs\n",
        "      self.l_rate = l_rate\n",
        "      self.params = self.initialization()\n",
        "\n",
        "  def softmax(self, x, derivative=False):\n",
        "          exps = np.exp(x - x.max())\n",
        "          if derivative:\n",
        "              return exps / np.sum(exps, axis=0) * (1 - exps / np.sum(exps, axis=0))\n",
        "          return exps / np.sum(exps, axis=0)\n",
        "\n",
        "  def sigmoid(self, x, derivative=False):\n",
        "          if derivative:\n",
        "              return (np.exp(-x))/((np.exp(-x)+1)**2)\n",
        "          return 1/(1 + np.exp(-x))\n",
        "\n",
        "  def ReLU(self, x, derivative = False):\n",
        "        if derivative:\n",
        "          y = x\n",
        "          y[y <= 0] = 0\n",
        "          y[y > 0] = 1\n",
        "          return y\n",
        "        x[x <= 0] = 0\n",
        "        return x\n",
        "\n",
        "\n",
        "  # with one hidden layer\n",
        "  def initialization(self):\n",
        "          # number of nodes in each layer\n",
        "        input_layer=self.sizes[0]\n",
        "        hidden_1=self.sizes[1]\n",
        "        hidden_2=self.sizes[2]\n",
        "        output_layer=self.sizes[3]\n",
        "\n",
        "        params = {\n",
        "            'W0':np.random.randn(hidden_1, input_layer) * np.sqrt(1. / hidden_1),\n",
        "            'W1':np.random.randn(hidden_2, hidden_1) * np.sqrt(1. / hidden_2),\n",
        "            'W2':np.random.randn(output_layer, hidden_2) * np.sqrt(1. / output_layer)\n",
        "        }\n",
        "\n",
        "        return params\n",
        "\n",
        "\n",
        "  def forward(self, x_train):\n",
        "        params = self.params\n",
        "        params['A0'] = x_train\n",
        "        # from input layer to hidden layer1\n",
        "        params['A1']=np.dot(params['W0'], params['A0'])\n",
        "        params['S1'] =self.ReLU(params['A1'])\n",
        "\n",
        "        # hidden layer1 to hidden layer2\n",
        "        params['A2'] = np.dot(params[\"W1\"], params['S1'])\n",
        "        params['S2'] = self.ReLU(params['A2'])\n",
        "\n",
        "        # hidden layer 2 to output layer\n",
        "        params['A3'] = np.dot(params[\"W2\"], params['S2'])\n",
        "        params['S3'] = self.softmax(params['A3'])\n",
        "\n",
        "        return params['S3']\n",
        "\n",
        "  def backward(self, y_train, output):\n",
        "        params = self.params\n",
        "        changes_to_w = {}\n",
        "\n",
        "        # Calculate W3 update\n",
        "        error = 2 * (output - y_train) / output.shape[0] * self.softmax(params['A3'], derivative=True)\n",
        "        changes_to_w['W2'] = np.outer(error, params['S2'])\n",
        "\n",
        "        # Calculate W1 update\n",
        "        error = np.dot(params['W2'].T, error) * self.ReLU(params['A2'],derivative=True)\n",
        "        changes_to_w['W1'] = np.outer(error, params['S1'])\n",
        "\n",
        "        # Calculate W0 update\n",
        "        error = np.dot(params['W1'].T, error) * self.ReLU(params['A1'],derivative=True)\n",
        "        changes_to_w['W0'] = np.outer(error, params['A0'])\n",
        "\n",
        "        return changes_to_w\n",
        "\n",
        "  def update_network_parameters(self, changes_to_w):        \n",
        "          for key, value in changes_to_w.items():\n",
        "              self.params[key] -= self.l_rate * value\n",
        "\n",
        "  def train(self, x_train, y_train, x_val, y_val):\n",
        "          start_time = time.time()\n",
        "          for iteration in range(self.epochs):\n",
        "              for x,y in zip(x_train, y_train):\n",
        "                  output = self.forward(x)\n",
        "                  changes_to_w = self.backward(y, output)\n",
        "                  self.update_network_parameters(changes_to_w)\n",
        "              \n",
        "              accuracy = self.evaluate_acc(x_val, y_val)\n",
        "              print('Epoch: {0}, Time Spent: {1:.2f}s, Accuracy: {2:.2f}%'.format(\n",
        "                  iteration+1, time.time() - start_time, accuracy * 100\n",
        "              ))\n",
        "  def evaluate_acc(self, x_val, y_val):\n",
        "          predictions = []\n",
        "\n",
        "          for x, y in zip(x_val, y_val):\n",
        "              output = self.forward(x)\n",
        "              pred = np.argmax(output)\n",
        "              predictions.append(pred == np.argmax(y))\n",
        "          \n",
        "          return np.mean(predictions)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JsAHZoEhbnLr",
        "outputId": "e000d583-e840-4819-a293-3afa14bcb2d8"
      },
      "source": [
        "mlp2 = MLP_2(sizes=[784,128,128,10])\n",
        "mlp2.train(x_train,y_train,x_train,y_train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 1, Time Spent: 60.90s, Accuracy: 80.86%\n",
            "Epoch: 2, Time Spent: 121.82s, Accuracy: 86.47%\n",
            "Epoch: 3, Time Spent: 182.58s, Accuracy: 88.49%\n",
            "Epoch: 4, Time Spent: 243.56s, Accuracy: 89.65%\n",
            "Epoch: 5, Time Spent: 304.15s, Accuracy: 90.44%\n",
            "Epoch: 6, Time Spent: 365.29s, Accuracy: 91.08%\n",
            "Epoch: 7, Time Spent: 426.79s, Accuracy: 91.58%\n",
            "Epoch: 8, Time Spent: 488.60s, Accuracy: 91.98%\n",
            "Epoch: 9, Time Spent: 549.66s, Accuracy: 92.28%\n",
            "Epoch: 10, Time Spent: 610.75s, Accuracy: 92.61%\n",
            "Epoch: 11, Time Spent: 671.41s, Accuracy: 92.87%\n",
            "Epoch: 12, Time Spent: 732.21s, Accuracy: 93.06%\n",
            "Epoch: 13, Time Spent: 792.84s, Accuracy: 93.29%\n",
            "Epoch: 14, Time Spent: 853.23s, Accuracy: 93.48%\n",
            "Epoch: 15, Time Spent: 913.83s, Accuracy: 93.62%\n",
            "Epoch: 16, Time Spent: 973.98s, Accuracy: 93.79%\n",
            "Epoch: 17, Time Spent: 1034.12s, Accuracy: 93.95%\n",
            "Epoch: 18, Time Spent: 1094.57s, Accuracy: 94.09%\n",
            "Epoch: 19, Time Spent: 1154.53s, Accuracy: 94.25%\n",
            "Epoch: 20, Time Spent: 1214.56s, Accuracy: 94.39%\n",
            "Epoch: 21, Time Spent: 1275.10s, Accuracy: 94.50%\n",
            "Epoch: 22, Time Spent: 1335.27s, Accuracy: 94.63%\n",
            "Epoch: 23, Time Spent: 1394.80s, Accuracy: 94.75%\n",
            "Epoch: 24, Time Spent: 1454.54s, Accuracy: 94.84%\n",
            "Epoch: 25, Time Spent: 1514.51s, Accuracy: 94.93%\n",
            "Epoch: 26, Time Spent: 1574.31s, Accuracy: 95.04%\n",
            "Epoch: 27, Time Spent: 1632.32s, Accuracy: 95.13%\n",
            "Epoch: 28, Time Spent: 1691.66s, Accuracy: 95.22%\n",
            "Epoch: 29, Time Spent: 1752.41s, Accuracy: 95.33%\n",
            "Epoch: 30, Time Spent: 1812.80s, Accuracy: 95.41%\n",
            "Epoch: 31, Time Spent: 1873.03s, Accuracy: 95.49%\n",
            "Epoch: 32, Time Spent: 1933.09s, Accuracy: 95.57%\n",
            "Epoch: 33, Time Spent: 1993.21s, Accuracy: 95.64%\n",
            "Epoch: 34, Time Spent: 2053.67s, Accuracy: 95.69%\n",
            "Epoch: 35, Time Spent: 2113.88s, Accuracy: 95.72%\n",
            "Epoch: 36, Time Spent: 2173.78s, Accuracy: 95.78%\n",
            "Epoch: 37, Time Spent: 2233.97s, Accuracy: 95.85%\n",
            "Epoch: 38, Time Spent: 2294.35s, Accuracy: 95.90%\n",
            "Epoch: 39, Time Spent: 2354.36s, Accuracy: 95.96%\n",
            "Epoch: 40, Time Spent: 2414.29s, Accuracy: 96.01%\n",
            "Epoch: 41, Time Spent: 2474.05s, Accuracy: 96.05%\n",
            "Epoch: 42, Time Spent: 2534.54s, Accuracy: 96.08%\n",
            "Epoch: 43, Time Spent: 2594.41s, Accuracy: 96.11%\n",
            "Epoch: 44, Time Spent: 2654.96s, Accuracy: 96.16%\n",
            "Epoch: 45, Time Spent: 2714.99s, Accuracy: 96.17%\n",
            "Epoch: 46, Time Spent: 2775.11s, Accuracy: 96.20%\n",
            "Epoch: 47, Time Spent: 2835.44s, Accuracy: 96.22%\n",
            "Epoch: 48, Time Spent: 2895.87s, Accuracy: 96.24%\n",
            "Epoch: 49, Time Spent: 2956.21s, Accuracy: 96.26%\n",
            "Epoch: 50, Time Spent: 3016.41s, Accuracy: 96.28%\n",
            "Epoch: 51, Time Spent: 3076.82s, Accuracy: 96.28%\n",
            "Epoch: 52, Time Spent: 3137.13s, Accuracy: 96.30%\n",
            "Epoch: 53, Time Spent: 3197.80s, Accuracy: 96.30%\n",
            "Epoch: 54, Time Spent: 3258.17s, Accuracy: 96.29%\n",
            "Epoch: 55, Time Spent: 3318.28s, Accuracy: 96.27%\n",
            "Epoch: 56, Time Spent: 3378.53s, Accuracy: 96.28%\n",
            "Epoch: 57, Time Spent: 3438.69s, Accuracy: 96.26%\n",
            "Epoch: 58, Time Spent: 3499.18s, Accuracy: 96.23%\n",
            "Epoch: 59, Time Spent: 3559.23s, Accuracy: 96.22%\n",
            "Epoch: 60, Time Spent: 3619.07s, Accuracy: 96.18%\n",
            "Epoch: 61, Time Spent: 3678.97s, Accuracy: 96.19%\n",
            "Epoch: 62, Time Spent: 3739.04s, Accuracy: 96.17%\n",
            "Epoch: 63, Time Spent: 3798.98s, Accuracy: 96.16%\n",
            "Epoch: 64, Time Spent: 3858.71s, Accuracy: 96.12%\n",
            "Epoch: 65, Time Spent: 3918.51s, Accuracy: 96.09%\n",
            "Epoch: 66, Time Spent: 3978.37s, Accuracy: 96.06%\n",
            "Epoch: 67, Time Spent: 4038.31s, Accuracy: 96.05%\n",
            "Epoch: 68, Time Spent: 4098.03s, Accuracy: 96.05%\n",
            "Epoch: 69, Time Spent: 4157.86s, Accuracy: 96.02%\n",
            "Epoch: 70, Time Spent: 4217.53s, Accuracy: 96.03%\n",
            "Epoch: 71, Time Spent: 4277.38s, Accuracy: 96.03%\n",
            "Epoch: 72, Time Spent: 4336.99s, Accuracy: 96.01%\n",
            "Epoch: 73, Time Spent: 4396.68s, Accuracy: 96.03%\n",
            "Epoch: 74, Time Spent: 4456.53s, Accuracy: 96.08%\n",
            "Epoch: 75, Time Spent: 4516.29s, Accuracy: 96.09%\n",
            "Epoch: 76, Time Spent: 4576.13s, Accuracy: 96.11%\n",
            "Epoch: 77, Time Spent: 4635.71s, Accuracy: 96.14%\n",
            "Epoch: 78, Time Spent: 4695.43s, Accuracy: 96.19%\n",
            "Epoch: 79, Time Spent: 4755.20s, Accuracy: 96.20%\n",
            "Epoch: 80, Time Spent: 4814.90s, Accuracy: 96.23%\n",
            "Epoch: 81, Time Spent: 4874.61s, Accuracy: 96.26%\n",
            "Epoch: 82, Time Spent: 4934.25s, Accuracy: 96.32%\n",
            "Epoch: 83, Time Spent: 4993.76s, Accuracy: 96.33%\n",
            "Epoch: 84, Time Spent: 5053.71s, Accuracy: 96.37%\n",
            "Epoch: 85, Time Spent: 5113.58s, Accuracy: 96.42%\n",
            "Epoch: 86, Time Spent: 5173.10s, Accuracy: 96.42%\n",
            "Epoch: 87, Time Spent: 5232.82s, Accuracy: 96.47%\n",
            "Epoch: 88, Time Spent: 5292.25s, Accuracy: 96.51%\n",
            "Epoch: 89, Time Spent: 5351.75s, Accuracy: 96.54%\n",
            "Epoch: 90, Time Spent: 5411.11s, Accuracy: 96.56%\n",
            "Epoch: 91, Time Spent: 5470.47s, Accuracy: 96.60%\n",
            "Epoch: 92, Time Spent: 5530.14s, Accuracy: 96.64%\n",
            "Epoch: 93, Time Spent: 5589.57s, Accuracy: 96.66%\n",
            "Epoch: 94, Time Spent: 5649.02s, Accuracy: 96.68%\n",
            "Epoch: 95, Time Spent: 5708.51s, Accuracy: 96.73%\n",
            "Epoch: 96, Time Spent: 5767.98s, Accuracy: 96.76%\n",
            "Epoch: 97, Time Spent: 5824.84s, Accuracy: 96.77%\n",
            "Epoch: 98, Time Spent: 5880.82s, Accuracy: 96.81%\n",
            "Epoch: 99, Time Spent: 5936.78s, Accuracy: 96.83%\n",
            "Epoch: 100, Time Spent: 5992.54s, Accuracy: 96.88%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uoO2unbqRmeF"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NWKp-bOP2FPP"
      },
      "source": [
        "\n",
        "class MLP_2_sigmoid():\n",
        "\n",
        "  # initialization\n",
        "  def __init__(self, sizes, epochs=100, l_rate=0.001):\n",
        "      self.sizes = sizes\n",
        "      self.epochs = epochs\n",
        "      self.l_rate = l_rate\n",
        "      self.params = self.initialization()\n",
        "\n",
        "  def softmax(self, x, derivative=False):\n",
        "          exps = np.exp(x - x.max())\n",
        "          if derivative:\n",
        "              return exps / np.sum(exps, axis=0) * (1 - exps / np.sum(exps, axis=0))\n",
        "          return exps / np.sum(exps, axis=0)\n",
        "\n",
        "  def sigmoid(self, x, derivative=False):\n",
        "          if derivative:\n",
        "              return (np.exp(-x))/((np.exp(-x)+1)**2)\n",
        "          return 1/(1 + np.exp(-x))\n",
        "\n",
        "  def ReLU(self, x, derivative = False):\n",
        "        if derivative:\n",
        "          y = x\n",
        "          y[y <= 0] = 0\n",
        "          y[y > 0] = 1\n",
        "          return y\n",
        "        x[x <= 0] = 0\n",
        "        return x\n",
        "\n",
        "\n",
        "  # with one hidden layer\n",
        "  def initialization(self):\n",
        "          # number of nodes in each layer\n",
        "        input_layer=self.sizes[0]\n",
        "        hidden_1=self.sizes[1]\n",
        "        hidden_2=self.sizes[2]\n",
        "        output_layer=self.sizes[3]\n",
        "\n",
        "        params = {\n",
        "            'W0':np.random.randn(hidden_1, input_layer) * np.sqrt(1. / hidden_1),\n",
        "            'W1':np.random.randn(hidden_2, hidden_1) * np.sqrt(1. / hidden_2),\n",
        "            'W2':np.random.randn(output_layer, hidden_2) * np.sqrt(1. / output_layer)\n",
        "        }\n",
        "\n",
        "        return params\n",
        "\n",
        "\n",
        "  def forward(self, x_train):\n",
        "        params = self.params\n",
        "        params['A0'] = x_train\n",
        "        # from input layer to hidden layer1\n",
        "        params['A1']=np.dot(params['W0'], params['A0'])\n",
        "        params['S1'] =self.sigmoid(params['A1'])\n",
        "\n",
        "        # hidden layer1 to hidden layer2\n",
        "        params['A2'] = np.dot(params[\"W1\"], params['S1'])\n",
        "        params['S2'] = self.sigmoid(params['A2'])\n",
        "\n",
        "        # hidden layer 2 to output layer\n",
        "        params['A3'] = np.dot(params[\"W2\"], params['S2'])\n",
        "        params['S3'] = self.softmax(params['A3'])\n",
        "\n",
        "        return params['S3']\n",
        "\n",
        "  def backward(self, y_train, output):\n",
        "        params = self.params\n",
        "        changes_to_w = {}\n",
        "\n",
        "        # Calculate W3 update\n",
        "        error = 2 * (output - y_train) / output.shape[0] * self.softmax(params['A3'], derivative=True)\n",
        "        changes_to_w['W2'] = np.outer(error, params['S2'])\n",
        "\n",
        "        # Calculate W1 update\n",
        "        error = np.dot(params['W2'].T, error) * self.sigmoid(params['A2'],derivative=True)\n",
        "        changes_to_w['W1'] = np.outer(error, params['S1'])\n",
        "\n",
        "        # Calculate W0 update\n",
        "        error = np.dot(params['W1'].T, error) * self.sigmoid(params['A1'],derivative=True)\n",
        "        changes_to_w['W0'] = np.outer(error, params['A0'])\n",
        "\n",
        "        return changes_to_w\n",
        "\n",
        "  def update_network_parameters(self, changes_to_w):        \n",
        "          for key, value in changes_to_w.items():\n",
        "              self.params[key] -= self.l_rate * value\n",
        "\n",
        "  def train(self, x_train, y_train, x_val, y_val):\n",
        "          start_time = time.time()\n",
        "          for iteration in range(self.epochs):\n",
        "              for x,y in zip(x_train, y_train):\n",
        "                  output = self.forward(x)\n",
        "                  changes_to_w = self.backward(y, output)\n",
        "                  self.update_network_parameters(changes_to_w)\n",
        "              \n",
        "              accuracy = self.evaluate_acc(x_val, y_val)\n",
        "              print('Epoch: {0}, Time Spent: {1:.2f}s, Accuracy: {2:.2f}%'.format(\n",
        "                  iteration+1, time.time() - start_time, accuracy * 100\n",
        "              ))\n",
        "  def evaluate_acc(self, x_val, y_val):\n",
        "          predictions = []\n",
        "\n",
        "          for x, y in zip(x_val, y_val):\n",
        "              output = self.forward(x)\n",
        "              pred = np.argmax(output)\n",
        "              predictions.append(pred == np.argmax(y))\n",
        "          \n",
        "          return np.mean(predictions)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jd7LxOwP3ELe",
        "outputId": "8817a6dd-f54e-4f3f-ed4a-ee00b065e63a"
      },
      "source": [
        "mlp2_sig = MLP_2_sigmoid(sizes=[784,128,128,10])\n",
        "mlp2_sig.train(x_train,y_train,x_train,y_train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 1, Time Spent: 70.56s, Accuracy: 29.03%\n",
            "Epoch: 2, Time Spent: 141.01s, Accuracy: 32.98%\n",
            "Epoch: 3, Time Spent: 207.09s, Accuracy: 36.14%\n",
            "Epoch: 4, Time Spent: 274.02s, Accuracy: 40.57%\n",
            "Epoch: 5, Time Spent: 341.56s, Accuracy: 46.52%\n",
            "Epoch: 6, Time Spent: 408.51s, Accuracy: 52.03%\n",
            "Epoch: 7, Time Spent: 477.72s, Accuracy: 55.99%\n",
            "Epoch: 8, Time Spent: 544.31s, Accuracy: 58.37%\n",
            "Epoch: 9, Time Spent: 610.89s, Accuracy: 60.05%\n",
            "Epoch: 10, Time Spent: 677.84s, Accuracy: 61.46%\n",
            "Epoch: 11, Time Spent: 746.37s, Accuracy: 62.61%\n",
            "Epoch: 12, Time Spent: 812.04s, Accuracy: 63.65%\n",
            "Epoch: 13, Time Spent: 877.56s, Accuracy: 64.60%\n",
            "Epoch: 14, Time Spent: 943.87s, Accuracy: 65.38%\n",
            "Epoch: 15, Time Spent: 1011.75s, Accuracy: 66.11%\n",
            "Epoch: 16, Time Spent: 1077.80s, Accuracy: 66.80%\n",
            "Epoch: 17, Time Spent: 1145.52s, Accuracy: 67.44%\n",
            "Epoch: 18, Time Spent: 1212.60s, Accuracy: 67.93%\n",
            "Epoch: 19, Time Spent: 1280.17s, Accuracy: 68.40%\n",
            "Epoch: 20, Time Spent: 1351.72s, Accuracy: 68.79%\n",
            "Epoch: 21, Time Spent: 1419.57s, Accuracy: 69.16%\n",
            "Epoch: 22, Time Spent: 1488.02s, Accuracy: 69.52%\n",
            "Epoch: 23, Time Spent: 1556.95s, Accuracy: 69.87%\n",
            "Epoch: 24, Time Spent: 1627.25s, Accuracy: 70.18%\n",
            "Epoch: 25, Time Spent: 1698.40s, Accuracy: 70.56%\n",
            "Epoch: 26, Time Spent: 1766.36s, Accuracy: 70.97%\n",
            "Epoch: 27, Time Spent: 1834.67s, Accuracy: 71.35%\n",
            "Epoch: 28, Time Spent: 1902.28s, Accuracy: 71.72%\n",
            "Epoch: 29, Time Spent: 1969.06s, Accuracy: 72.03%\n",
            "Epoch: 30, Time Spent: 2036.00s, Accuracy: 72.31%\n",
            "Epoch: 31, Time Spent: 2102.83s, Accuracy: 72.62%\n",
            "Epoch: 32, Time Spent: 2169.78s, Accuracy: 72.95%\n",
            "Epoch: 33, Time Spent: 2236.31s, Accuracy: 73.29%\n",
            "Epoch: 34, Time Spent: 2304.19s, Accuracy: 73.66%\n",
            "Epoch: 35, Time Spent: 2371.26s, Accuracy: 74.09%\n",
            "Epoch: 36, Time Spent: 2438.48s, Accuracy: 74.42%\n",
            "Epoch: 37, Time Spent: 2505.51s, Accuracy: 74.77%\n",
            "Epoch: 38, Time Spent: 2572.36s, Accuracy: 75.09%\n",
            "Epoch: 39, Time Spent: 2642.33s, Accuracy: 75.46%\n",
            "Epoch: 40, Time Spent: 2712.65s, Accuracy: 75.73%\n",
            "Epoch: 41, Time Spent: 2783.63s, Accuracy: 76.11%\n",
            "Epoch: 42, Time Spent: 2855.31s, Accuracy: 76.51%\n",
            "Epoch: 43, Time Spent: 2924.66s, Accuracy: 76.91%\n",
            "Epoch: 44, Time Spent: 2999.68s, Accuracy: 77.33%\n",
            "Epoch: 45, Time Spent: 3071.76s, Accuracy: 77.66%\n",
            "Epoch: 46, Time Spent: 3141.67s, Accuracy: 78.07%\n",
            "Epoch: 47, Time Spent: 3211.93s, Accuracy: 78.45%\n",
            "Epoch: 48, Time Spent: 3283.94s, Accuracy: 78.63%\n",
            "Epoch: 49, Time Spent: 3356.22s, Accuracy: 78.78%\n",
            "Epoch: 50, Time Spent: 3427.37s, Accuracy: 78.90%\n",
            "Epoch: 51, Time Spent: 3500.11s, Accuracy: 78.93%\n",
            "Epoch: 52, Time Spent: 3574.59s, Accuracy: 78.96%\n",
            "Epoch: 53, Time Spent: 3649.29s, Accuracy: 78.97%\n",
            "Epoch: 54, Time Spent: 3724.18s, Accuracy: 78.97%\n",
            "Epoch: 55, Time Spent: 3795.66s, Accuracy: 79.01%\n",
            "Epoch: 56, Time Spent: 3866.13s, Accuracy: 79.01%\n",
            "Epoch: 57, Time Spent: 3938.75s, Accuracy: 79.04%\n",
            "Epoch: 58, Time Spent: 4009.53s, Accuracy: 79.02%\n",
            "Epoch: 59, Time Spent: 4080.55s, Accuracy: 79.00%\n",
            "Epoch: 60, Time Spent: 4151.84s, Accuracy: 79.01%\n",
            "Epoch: 61, Time Spent: 4223.78s, Accuracy: 79.02%\n",
            "Epoch: 62, Time Spent: 4296.73s, Accuracy: 79.02%\n",
            "Epoch: 63, Time Spent: 4368.87s, Accuracy: 79.07%\n",
            "Epoch: 64, Time Spent: 4440.36s, Accuracy: 79.10%\n",
            "Epoch: 65, Time Spent: 4511.30s, Accuracy: 79.13%\n",
            "Epoch: 66, Time Spent: 4579.98s, Accuracy: 79.14%\n",
            "Epoch: 67, Time Spent: 4649.95s, Accuracy: 79.19%\n",
            "Epoch: 68, Time Spent: 4719.69s, Accuracy: 79.25%\n",
            "Epoch: 69, Time Spent: 4788.95s, Accuracy: 79.31%\n",
            "Epoch: 70, Time Spent: 4857.35s, Accuracy: 79.35%\n",
            "Epoch: 71, Time Spent: 4928.02s, Accuracy: 79.42%\n",
            "Epoch: 72, Time Spent: 4996.63s, Accuracy: 79.46%\n",
            "Epoch: 73, Time Spent: 5064.20s, Accuracy: 79.47%\n",
            "Epoch: 74, Time Spent: 5131.52s, Accuracy: 79.53%\n",
            "Epoch: 75, Time Spent: 5199.01s, Accuracy: 79.57%\n",
            "Epoch: 76, Time Spent: 5267.15s, Accuracy: 79.65%\n",
            "Epoch: 77, Time Spent: 5333.79s, Accuracy: 79.72%\n",
            "Epoch: 78, Time Spent: 5400.37s, Accuracy: 79.79%\n",
            "Epoch: 79, Time Spent: 5468.71s, Accuracy: 79.86%\n",
            "Epoch: 80, Time Spent: 5536.09s, Accuracy: 79.90%\n",
            "Epoch: 81, Time Spent: 5604.66s, Accuracy: 79.95%\n",
            "Epoch: 82, Time Spent: 5672.86s, Accuracy: 80.03%\n",
            "Epoch: 83, Time Spent: 5741.60s, Accuracy: 80.07%\n",
            "Epoch: 84, Time Spent: 5809.02s, Accuracy: 80.14%\n",
            "Epoch: 85, Time Spent: 5876.79s, Accuracy: 80.19%\n",
            "Epoch: 86, Time Spent: 5944.04s, Accuracy: 80.24%\n",
            "Epoch: 87, Time Spent: 6012.59s, Accuracy: 80.28%\n",
            "Epoch: 88, Time Spent: 6079.27s, Accuracy: 80.35%\n",
            "Epoch: 89, Time Spent: 6144.40s, Accuracy: 80.44%\n",
            "Epoch: 90, Time Spent: 6211.98s, Accuracy: 80.49%\n",
            "Epoch: 91, Time Spent: 6278.71s, Accuracy: 80.53%\n",
            "Epoch: 92, Time Spent: 6344.61s, Accuracy: 80.59%\n",
            "Epoch: 93, Time Spent: 6410.65s, Accuracy: 80.63%\n",
            "Epoch: 94, Time Spent: 6477.41s, Accuracy: 80.71%\n",
            "Epoch: 95, Time Spent: 6543.72s, Accuracy: 80.80%\n",
            "Epoch: 96, Time Spent: 6608.94s, Accuracy: 80.90%\n",
            "Epoch: 97, Time Spent: 6673.97s, Accuracy: 81.05%\n",
            "Epoch: 98, Time Spent: 6739.18s, Accuracy: 81.24%\n",
            "Epoch: 99, Time Spent: 6804.29s, Accuracy: 81.51%\n",
            "Epoch: 100, Time Spent: 6872.17s, Accuracy: 81.85%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "72sNL8jmRr02"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cPnUX_QFpY78"
      },
      "source": [
        "class MLP_2_tanh():\n",
        "\n",
        "# initialization\n",
        "  def __init__(self, sizes, epochs=100, l_rate=0.001):\n",
        "      self.sizes = sizes\n",
        "      self.epochs = epochs\n",
        "      self.l_rate = l_rate\n",
        "      self.params = self.initialization()\n",
        "\n",
        "  def softmax(self, x, derivative=False):\n",
        "          exps = np.exp(x - x.max())\n",
        "          if derivative:\n",
        "              return exps / np.sum(exps, axis=0) * (1 - exps / np.sum(exps, axis=0))\n",
        "          return exps / np.sum(exps, axis=0)\n",
        "\n",
        "  def sigmoid(self, x, derivative=False):\n",
        "          if derivative:\n",
        "              return (np.exp(-x))/((np.exp(-x)+1)**2)\n",
        "          return 1/(1 + np.exp(-x))\n",
        "\n",
        "  def ReLU(self, x, derivative = False):\n",
        "        if derivative:\n",
        "          y = x\n",
        "          y[y <= 0] = 0\n",
        "          y[y > 0] = 1\n",
        "          return y\n",
        "        x[x <= 0] = 0\n",
        "        return x\n",
        "  def tanh(self, x, derivative = False):\n",
        "        t=(np.exp(x)-np.exp(-x))/(np.exp(x)+np.exp(-x))\n",
        "        dt=1-t**2\n",
        "        if derivative:\n",
        "          return dt\n",
        "        return t\n",
        "\n",
        "\n",
        "  # with one hidden layer\n",
        "  def initialization(self):\n",
        "          # number of nodes in each layer\n",
        "        input_layer=self.sizes[0]\n",
        "        hidden_1=self.sizes[1]\n",
        "        hidden_2=self.sizes[2]\n",
        "        output_layer=self.sizes[3]\n",
        "\n",
        "        params = {\n",
        "            'W0':np.random.randn(hidden_1, input_layer) * np.sqrt(1. / hidden_1),\n",
        "            'W1':np.random.randn(hidden_2, hidden_1) * np.sqrt(1. / hidden_2),\n",
        "            'W2':np.random.randn(output_layer, hidden_2) * np.sqrt(1. / output_layer)\n",
        "        }\n",
        "\n",
        "        return params\n",
        "\n",
        "\n",
        "  def forward(self, x_train):\n",
        "        params = self.params\n",
        "        params['A0'] = x_train\n",
        "        # from input layer to hidden layer1\n",
        "        params['A1']=np.dot(params['W0'], params['A0'])\n",
        "        params['S1'] =self.tanh(params['A1'])\n",
        "\n",
        "        # hidden layer1 to hidden layer2\n",
        "        params['A2'] = np.dot(params[\"W1\"], params['S1'])\n",
        "        params['S2'] = self.tanh(params['A2'])\n",
        "\n",
        "        # hidden layer 2 to output layer\n",
        "        params['A3'] = np.dot(params[\"W2\"], params['S2'])\n",
        "        params['S3'] = self.softmax(params['A3'])\n",
        "\n",
        "        return params['S3']\n",
        "\n",
        "  def backward(self, y_train, output):\n",
        "        params = self.params\n",
        "        changes_to_w = {}\n",
        "\n",
        "        # Calculate W3 update\n",
        "        error = 2 * (output - y_train) / output.shape[0] * self.softmax(params['A3'], derivative=True)\n",
        "        changes_to_w['W2'] = np.outer(error, params['S2'])\n",
        "\n",
        "        # Calculate W1 update\n",
        "        error = np.dot(params['W2'].T, error) * self.tanh(params['A2'],derivative=True)\n",
        "        changes_to_w['W1'] = np.outer(error, params['S1'])\n",
        "\n",
        "        # Calculate W0 update\n",
        "        error = np.dot(params['W1'].T, error) * self.tanh(params['A1'],derivative=True)\n",
        "        changes_to_w['W0'] = np.outer(error, params['A0'])\n",
        "\n",
        "        return changes_to_w\n",
        "\n",
        "  def update_network_parameters(self, changes_to_w):        \n",
        "          for key, value in changes_to_w.items():\n",
        "              self.params[key] -= self.l_rate * value\n",
        "\n",
        "  def train(self, x_train, y_train, x_val, y_val):\n",
        "          start_time = time.time()\n",
        "          for iteration in range(self.epochs):\n",
        "              for x,y in zip(x_train, y_train):\n",
        "                  output = self.forward(x)\n",
        "                  changes_to_w = self.backward(y, output)\n",
        "                  self.update_network_parameters(changes_to_w)\n",
        "              \n",
        "              accuracy = self.evaluate_acc(x_val, y_val)\n",
        "              print('Epoch: {0}, Time Spent: {1:.2f}s, Accuracy: {2:.2f}%'.format(\n",
        "                  iteration+1, time.time() - start_time, accuracy * 100\n",
        "              ))\n",
        "  def evaluate_acc(self, x_val, y_val):\n",
        "          predictions = []\n",
        "\n",
        "          for x, y in zip(x_val, y_val):\n",
        "              output = self.forward(x)\n",
        "              pred = np.argmax(output)\n",
        "              predictions.append(pred == np.argmax(y))\n",
        "          return np.mean(predictions)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xhxep1RPEBB2",
        "outputId": "088a704f-50ae-43fe-92e9-0aa4488b1525"
      },
      "source": [
        "mlp2_tanh = MLP_2_tanh(sizes=[784,128,128,10])\n",
        "mlp2_tanh.train(x_train,y_train,x_train,y_train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 1, Time Spent: 67.98s, Accuracy: 82.53%\n",
            "Epoch: 2, Time Spent: 135.83s, Accuracy: 86.48%\n",
            "Epoch: 3, Time Spent: 203.55s, Accuracy: 88.11%\n",
            "Epoch: 4, Time Spent: 271.24s, Accuracy: 89.17%\n",
            "Epoch: 5, Time Spent: 338.92s, Accuracy: 89.91%\n",
            "Epoch: 6, Time Spent: 406.45s, Accuracy: 90.45%\n",
            "Epoch: 7, Time Spent: 473.87s, Accuracy: 90.92%\n",
            "Epoch: 8, Time Spent: 541.26s, Accuracy: 91.28%\n",
            "Epoch: 9, Time Spent: 608.77s, Accuracy: 91.61%\n",
            "Epoch: 10, Time Spent: 676.25s, Accuracy: 91.90%\n",
            "Epoch: 11, Time Spent: 743.90s, Accuracy: 92.09%\n",
            "Epoch: 12, Time Spent: 811.43s, Accuracy: 92.31%\n",
            "Epoch: 13, Time Spent: 879.17s, Accuracy: 92.53%\n",
            "Epoch: 14, Time Spent: 946.89s, Accuracy: 92.74%\n",
            "Epoch: 15, Time Spent: 1014.73s, Accuracy: 92.92%\n",
            "Epoch: 16, Time Spent: 1082.15s, Accuracy: 93.08%\n",
            "Epoch: 17, Time Spent: 1149.54s, Accuracy: 93.23%\n",
            "Epoch: 18, Time Spent: 1217.15s, Accuracy: 93.37%\n",
            "Epoch: 19, Time Spent: 1284.84s, Accuracy: 93.51%\n",
            "Epoch: 20, Time Spent: 1352.10s, Accuracy: 93.61%\n",
            "Epoch: 21, Time Spent: 1419.46s, Accuracy: 93.73%\n",
            "Epoch: 22, Time Spent: 1487.11s, Accuracy: 93.87%\n",
            "Epoch: 23, Time Spent: 1554.47s, Accuracy: 93.96%\n",
            "Epoch: 24, Time Spent: 1621.90s, Accuracy: 94.06%\n",
            "Epoch: 25, Time Spent: 1689.30s, Accuracy: 94.17%\n",
            "Epoch: 26, Time Spent: 1756.82s, Accuracy: 94.25%\n",
            "Epoch: 27, Time Spent: 1824.60s, Accuracy: 94.36%\n",
            "Epoch: 28, Time Spent: 1892.10s, Accuracy: 94.45%\n",
            "Epoch: 29, Time Spent: 1959.57s, Accuracy: 94.53%\n",
            "Epoch: 30, Time Spent: 2026.88s, Accuracy: 94.63%\n",
            "Epoch: 31, Time Spent: 2094.33s, Accuracy: 94.69%\n",
            "Epoch: 32, Time Spent: 2161.82s, Accuracy: 94.76%\n",
            "Epoch: 33, Time Spent: 2229.28s, Accuracy: 94.84%\n",
            "Epoch: 34, Time Spent: 2293.05s, Accuracy: 94.92%\n",
            "Epoch: 35, Time Spent: 2356.68s, Accuracy: 94.99%\n",
            "Epoch: 36, Time Spent: 2424.66s, Accuracy: 95.06%\n",
            "Epoch: 37, Time Spent: 2492.72s, Accuracy: 95.12%\n",
            "Epoch: 38, Time Spent: 2560.77s, Accuracy: 95.18%\n",
            "Epoch: 39, Time Spent: 2628.74s, Accuracy: 95.24%\n",
            "Epoch: 40, Time Spent: 2696.32s, Accuracy: 95.30%\n",
            "Epoch: 41, Time Spent: 2763.72s, Accuracy: 95.35%\n",
            "Epoch: 42, Time Spent: 2831.13s, Accuracy: 95.40%\n",
            "Epoch: 43, Time Spent: 2898.45s, Accuracy: 95.45%\n",
            "Epoch: 44, Time Spent: 2965.96s, Accuracy: 95.50%\n",
            "Epoch: 45, Time Spent: 3033.44s, Accuracy: 95.56%\n",
            "Epoch: 46, Time Spent: 3100.80s, Accuracy: 95.59%\n",
            "Epoch: 47, Time Spent: 3168.27s, Accuracy: 95.65%\n",
            "Epoch: 48, Time Spent: 3235.69s, Accuracy: 95.70%\n",
            "Epoch: 49, Time Spent: 3302.98s, Accuracy: 95.74%\n",
            "Epoch: 50, Time Spent: 3370.49s, Accuracy: 95.79%\n",
            "Epoch: 51, Time Spent: 3438.17s, Accuracy: 95.84%\n",
            "Epoch: 52, Time Spent: 3505.69s, Accuracy: 95.88%\n",
            "Epoch: 53, Time Spent: 3573.12s, Accuracy: 95.91%\n",
            "Epoch: 54, Time Spent: 3640.61s, Accuracy: 95.94%\n",
            "Epoch: 55, Time Spent: 3708.15s, Accuracy: 95.97%\n",
            "Epoch: 56, Time Spent: 3775.38s, Accuracy: 96.02%\n",
            "Epoch: 57, Time Spent: 3841.96s, Accuracy: 96.06%\n",
            "Epoch: 58, Time Spent: 3908.82s, Accuracy: 96.10%\n",
            "Epoch: 59, Time Spent: 3975.85s, Accuracy: 96.12%\n",
            "Epoch: 60, Time Spent: 4042.42s, Accuracy: 96.17%\n",
            "Epoch: 61, Time Spent: 4109.18s, Accuracy: 96.21%\n",
            "Epoch: 62, Time Spent: 4175.87s, Accuracy: 96.23%\n",
            "Epoch: 63, Time Spent: 4242.40s, Accuracy: 96.28%\n",
            "Epoch: 64, Time Spent: 4309.17s, Accuracy: 96.30%\n",
            "Epoch: 65, Time Spent: 4375.90s, Accuracy: 96.33%\n",
            "Epoch: 66, Time Spent: 4442.64s, Accuracy: 96.35%\n",
            "Epoch: 67, Time Spent: 4509.17s, Accuracy: 96.38%\n",
            "Epoch: 68, Time Spent: 4575.84s, Accuracy: 96.40%\n",
            "Epoch: 69, Time Spent: 4642.81s, Accuracy: 96.44%\n",
            "Epoch: 70, Time Spent: 4709.90s, Accuracy: 96.45%\n",
            "Epoch: 71, Time Spent: 4776.83s, Accuracy: 96.50%\n",
            "Epoch: 72, Time Spent: 4843.84s, Accuracy: 96.54%\n",
            "Epoch: 73, Time Spent: 4910.98s, Accuracy: 96.56%\n",
            "Epoch: 74, Time Spent: 4978.12s, Accuracy: 96.60%\n",
            "Epoch: 75, Time Spent: 5045.23s, Accuracy: 96.64%\n",
            "Epoch: 76, Time Spent: 5112.11s, Accuracy: 96.67%\n",
            "Epoch: 77, Time Spent: 5179.24s, Accuracy: 96.69%\n",
            "Epoch: 78, Time Spent: 5246.32s, Accuracy: 96.71%\n",
            "Epoch: 79, Time Spent: 5313.30s, Accuracy: 96.74%\n",
            "Epoch: 80, Time Spent: 5380.35s, Accuracy: 96.77%\n",
            "Epoch: 81, Time Spent: 5447.21s, Accuracy: 96.79%\n",
            "Epoch: 82, Time Spent: 5513.89s, Accuracy: 96.81%\n",
            "Epoch: 83, Time Spent: 5580.68s, Accuracy: 96.83%\n",
            "Epoch: 84, Time Spent: 5647.20s, Accuracy: 96.85%\n",
            "Epoch: 85, Time Spent: 5713.74s, Accuracy: 96.88%\n",
            "Epoch: 86, Time Spent: 5780.21s, Accuracy: 96.90%\n",
            "Epoch: 87, Time Spent: 5846.84s, Accuracy: 96.92%\n",
            "Epoch: 88, Time Spent: 5913.54s, Accuracy: 96.95%\n",
            "Epoch: 89, Time Spent: 5980.12s, Accuracy: 96.98%\n",
            "Epoch: 90, Time Spent: 6046.69s, Accuracy: 97.01%\n",
            "Epoch: 91, Time Spent: 6113.28s, Accuracy: 97.03%\n",
            "Epoch: 92, Time Spent: 6179.94s, Accuracy: 97.05%\n",
            "Epoch: 93, Time Spent: 6246.62s, Accuracy: 97.08%\n",
            "Epoch: 94, Time Spent: 6313.20s, Accuracy: 97.09%\n",
            "Epoch: 95, Time Spent: 6379.80s, Accuracy: 97.11%\n",
            "Epoch: 96, Time Spent: 6446.35s, Accuracy: 97.13%\n",
            "Epoch: 97, Time Spent: 6513.04s, Accuracy: 97.14%\n",
            "Epoch: 98, Time Spent: 6579.60s, Accuracy: 97.15%\n",
            "Epoch: 99, Time Spent: 6645.90s, Accuracy: 97.16%\n",
            "Epoch: 100, Time Spent: 6712.32s, Accuracy: 97.18%\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}